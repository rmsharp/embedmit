---
title: "Encoding Categorical Data"
subtitle: "Effect Encodings with embedmit"
author: "Based on Chapter 17 of Tidy Modeling with R"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Encoding Categorical Data (R Markdown)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  message = FALSE,
  warning = FALSE
)

# Check for required packages (including embedmit itself for vignette building)
required_pkgs <- c("embedmit", "modeldata", "ggplot2", "lme4", "dplyr", "rsample", "purrr")
have_pkgs <- vapply(required_pkgs, requireNamespace, logical(1), quietly = TRUE)
if (!all(have_pkgs)) {
  knitr::opts_chunk$set(eval = FALSE)
}
```

This vignette demonstrates categorical encoding techniques using `embedmit`, closely following [Chapter 17 of Tidy Modeling with R](https://www.tmwr.org/categorical) by Max Kuhn and Julia Silge.

## Introduction

For statistical modeling in R, the preferred representation for categorical or nominal data is a *factor*, which is a variable that can take on a limited number of different values. Internally, factors are stored as a vector of integer values together with a set of text labels.

The most straightforward approach for transforming a categorical variable to a numeric representation is to create dummy or indicator variables from the levels. However, this approach does not work well when you have a variable with high cardinality (too many levels) or when you may encounter novel values at prediction time (new levels).

This vignette explores alternative encoding strategies that address these challenges.

## Setup

```{r libraries}
library(embedmit)
library(recipes)
library(dplyr)
library(rsample)
library(ggplot2)
library(purrr)
library(modeldata)

# Load the Ames housing data
data(ames)

# Create train/test split
set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

## Is an Encoding Necessary?

A minority of models, such as those based on trees or rules, can handle categorical data natively and do not require encoding or transformation of these kinds of features. For example:

- **Tree-based models** (decision trees, random forests, boosted trees) can find optimal splits on categorical variables directly
- **Naive Bayes** models compute class probabilities without requiring numeric encoding

For these models, research has shown that creating dummy variables typically does not improve performance and can increase computation time. The `recipes` package provides `step_dummy()` for standard dummy encoding when it is needed.

## Using the Outcome for Encoding Predictors

There are multiple options for encodings more complex than dummy or indicator variables. One method called *effect* or *likelihood encodings* replaces the original categorical variables with a single numeric column that measures the effect of those data.

For example, for the Ames housing data, we can compute the mean or median sale price for each neighborhood and use this value to represent that categorical level. Effect encodings can also seamlessly handle situations where a novel factor level is encountered in the data.

### Visualizing Neighborhood Effects

The Ames data has 28 different neighborhoods. Let's visualize how sale price varies across them:

```{r neighborhood-viz, fig.cap="Mean sale price by neighborhood with 90% confidence intervals. Neighborhoods are ordered by their mean sale price."}
ames_train %>%
  group_by(Neighborhood) %>%
  summarize(
    mean = mean(Sale_Price),
    std_err = sd(Sale_Price) / sqrt(length(Sale_Price)),
    .groups = "drop"
  ) %>%
  ggplot(aes(y = reorder(Neighborhood, mean), x = mean)) +
  geom_point() +
  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err)) +
  labs(y = NULL, x = "Price (mean, log scale)") +
  theme_minimal()
```

### GLM-Based Effect Encoding

The `step_lencode_glm()` function uses a generalized linear model to estimate the effect of each category level:

```{r glm-recipe}
ames_glm <-
recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>%
  step_lencode_glm(Neighborhood, outcome = vars(Sale_Price)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_")) %>%
  step_ns(Latitude, Longitude, deg_free = 20)

ames_glm
```

We can examine the learned encodings by preparing the recipe and using `tidy()`:

```{r glm-estimates}
glm_estimates <-
  prep(ames_glm) %>%
  tidy(number = 2)

glm_estimates
```

Each neighborhood is replaced by a single numeric value representing its effect on sale price.

### Handling Novel Categories

Effect encodings can seamlessly handle situations where a novel factor level is encountered in the data. The encoding includes a special `..new` level:

```{r novel-levels}
glm_estimates %>%
  filter(level == "..new")
```

When the model encounters an unseen neighborhood at prediction time, it uses this default encoding.

## Effect Encodings with Partial Pooling

Creating an effect encoding with `step_lencode_glm()` estimates the effect separately for each factor level (in this case, neighborhood). However, some of these neighborhoods have many houses in them, and some have only a few. There is much more uncertainty in our measurement of price for neighborhoods with few training set homes than for neighborhoods with many.

We can use *partial pooling* to adjust these estimates so that levels with small sample sizes are shrunk toward the overall mean. The `step_lencode_mixed()` function uses hierarchical or mixed effects models to accomplish this:

```{r mixed-recipe}
ames_mixed <-
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>%
  step_lencode_mixed(Neighborhood, outcome = vars(Sale_Price)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_")) %>%
  step_ns(Latitude, Longitude, deg_free = 20)

ames_mixed
```

```{r mixed-estimates}
mixed_estimates <-
  prep(ames_mixed) %>%
  tidy(number = 2)

mixed_estimates
```

```{r mixed-novel}
mixed_estimates %>%
  filter(level == "..new")
```

### Comparing Pooling Methods

Let's visualize how partial pooling affects the estimates:

```{r compare-pooling, fig.cap="Comparison of GLM (no pooling) versus mixed effects (partial pooling) encodings. Point size represents the number of observations in each neighborhood."}
glm_estimates %>%
  rename(`no pooling` = value) %>%
  left_join(
    mixed_estimates %>%
      rename(`partial pooling` = value),
    by = "level"
  ) %>%
  left_join(
    ames_train %>%
      count(Neighborhood) %>%
      mutate(level = as.character(Neighborhood)),
    by = "level"
  ) %>%
  filter(!is.na(n)) %>%
  ggplot(aes(`no pooling`, `partial pooling`, size = sqrt(n))) +
  geom_abline(color = "gray50", lty = 2) +
  geom_point(alpha = 0.7) +
  coord_fixed() +
  labs(size = "sqrt(n)") +
  theme_minimal()
```

When we use partial pooling, we shrink the effect estimates toward the mean because we don't have as much evidence about the price in neighborhoods with few observations. Neighborhoods with many observations retain estimates close to the unpooled GLM values.

### Bayesian Effect Encoding

For fully Bayesian uncertainty quantification, `step_lencode_bayes()` provides an alternative approach (requires the `rstanarm` package):

```{r bayes-recipe, eval=FALSE}
ames_bayes <-
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>%
  step_lencode_bayes(Neighborhood, outcome = vars(Sale_Price)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_")) %>%
  step_ns(Latitude, Longitude, deg_free = 20)
```

## Feature Hashing

*Feature hashing* methods also create dummy variables, but only consider the value of the category to assign it to a predefined pool of dummy variables. A hashing function takes an input of variable size and maps it to an output of fixed size.

```{r hashing-demo}
library(rlang)

ames_hashed <-
  ames_train %>%
  mutate(Hash = map_chr(Neighborhood, hash))

ames_hashed %>%
  select(Neighborhood, Hash) %>%
  head(6)
```

In feature hashing, the number of possible hashes is a hyperparameter and is set by the model developer through computing the modulo of the integer hashes:

```{r hashing-modulo}
ames_hashed %>%
  mutate(
    Hash = strtoi(substr(Hash, 26, 32), base = 16L),
    Hash = Hash %% 16
  ) %>%
  select(Neighborhood, Hash) %>%
  head(10)
```

Feature hashing can handle new category levels at prediction time, since it does not rely on pre-determined dummy variables. The `textrecipes` package provides `step_dummy_hash()` for this approach.

## More Encoding Options

The `embedmit` package offers additional encoding methods:

| Function | Description |
|----------|-------------|
| `step_lencode_glm()` | GLM-based effect encoding (no pooling) |
| `step_lencode_mixed()` | Mixed effects encoding (partial pooling) |
| `step_lencode_bayes()` | Bayesian encoding (full uncertainty quantification) |
| `step_woe()` | Weight of evidence transformation (binary outcomes) |
| `step_umap()` | UMAP embeddings via uwotlite |

## Chapter Summary

The most straightforward option for transforming a categorical variable to a numeric representation is to create dummy variables from the levels, but this option does not work well when you have a variable with high cardinality (too many levels) or when you may see novel values at prediction time (new levels).

Effect encodings and feature hashing address these challenges:

- **Effect encodings** replace categories with a single numeric value measuring the outcome relationship
- **Partial pooling** (via `step_lencode_mixed()`) adjusts estimates so that levels with small sample sizes are shrunk toward the overall mean
- **Feature hashing** maps categories to a predefined pool of dummy variables and can handle novel categories

Other options include entity embeddings (learned via a neural network with `step_embed()`) and weight of evidence transformation (via `step_woe()`).

## References

- Kuhn, M., & Johnson, K. (2019). *Feature Engineering and Selection*. CRC Press.
- Kuhn, M., & Silge, J. (2022). *Tidy Modeling with R*. O'Reilly Media.
- Micci-Barreca, D. (2001). A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems. *ACM SIGKDD Explorations*, 3(1), 27-32.
