---
title: "Encoding Categorical Data with embedmit"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Encoding Categorical Data with embedmit}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)

# Check for required packages
required_pkgs <- c("modeldata", "ggplot2", "lme4")
have_pkgs <- vapply(required_pkgs, requireNamespace, logical(1), quietly = TRUE)
if (!all(have_pkgs)) {
knitr::opts_chunk$set(eval = FALSE)
}
```

This vignette demonstrates categorical encoding techniques using `embedmit`,
following the approach from [Chapter 17 of Tidy Modeling with R](https://www.tmwr.org/categorical).

The `embedmit` package provides MIT-licensed effect encoding methods that transform
categorical variables into numeric representations suitable for machine learning models.

## Setup

```{r libraries, message=FALSE, warning=FALSE}
library(embedmit)
library(recipes)
library(dplyr)
library(rsample)
library(modeldata)

# Load the Ames housing data
data(ames)

# Create train/test split
set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

## The Challenge: High-Cardinality Categorical Variables

The Ames dataset contains neighborhood information with many levels:

```{r neighborhood-levels}
ames_train %>%
  count(Neighborhood, sort = TRUE) %>%
  print(n = 30)
```

Creating dummy variables for all these levels leads to many columns and potential
overfitting. Effect encoding offers an alternative approach.

## Effect Encoding Methods

Effect encoding replaces each categorical level with a single numeric value
that represents the relationship between that level and the outcome. This
reduces a high-cardinality categorical variable to a single numeric column.

### Method 1: GLM-based Effect Encoding

The `step_lencode_glm()` function uses a generalized linear model to estimate
the effect of each category level:

```{r glm-encoding}
ames_glm <-
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>%
  step_lencode_glm(Neighborhood, outcome = vars(Sale_Price)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_")) %>%
  step_ns(Latitude, Longitude, deg_free = 20)

# Prepare the recipe and examine the encodings
glm_prep <- prep(ames_glm)
glm_estimates <- tidy(glm_prep, number = 2)

glm_estimates %>%
  arrange(desc(value)) %>%
  print(n = 15)
```

The GLM method produces separate estimates for each neighborhood level. Levels
associated with higher sale prices get higher encoded values.

### Handling Novel Categories

A key advantage of effect encoding is handling categories not seen during training:

```{r novel-levels}
# The "..new" level represents the encoding for unseen categories
glm_estimates %>%
  filter(level == "..new")
```

### Method 2: Mixed Effects Encoding (Partial Pooling)

The `step_lencode_mixed()` function uses mixed effects models that apply
**partial pooling**. This shrinks estimates toward the overall mean, especially
for categories with few observations:

```{r mixed-encoding}
ames_mixed <-
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>%
  step_lencode_mixed(Neighborhood, outcome = vars(Sale_Price)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_")) %>%
  step_ns(Latitude, Longitude, deg_free = 20)

mixed_prep <- prep(ames_mixed)
mixed_estimates <- tidy(mixed_prep, number = 2)

mixed_estimates %>%
  arrange(desc(value)) %>%
  print(n = 15)
```

### Comparing GLM vs Mixed Effects

Let's visualize how partial pooling affects the estimates:

```{r compare-methods, fig.cap="Comparison of GLM (no pooling) vs Mixed Effects (partial pooling) encodings. Point size represents sample size for each neighborhood."}
comparison <- glm_estimates %>%
  rename(`no pooling` = value) %>%
  left_join(
    mixed_estimates %>%
      rename(`partial pooling` = value),
    by = "level"
  ) %>%
  left_join(
    ames_train %>%
      count(Neighborhood) %>%
      mutate(level = as.character(Neighborhood)),
    by = "level"
  ) %>%
  filter(level != "..new")

library(ggplot2)

ggplot(comparison, aes(`no pooling`, `partial pooling`, size = sqrt(n))) +

geom_abline(color = "gray50", lty = 2) +
  geom_point(alpha = 0.7) +
  coord_fixed() +
  labs(
    title = "Effect of Partial Pooling on Neighborhood Encodings",
    subtitle = "Points off the diagonal show shrinkage toward the mean",
    size = "sqrt(n)"
  ) +
  theme_minimal()
```

Small neighborhoods (smaller points) show more shrinkage toward the diagonal,
while large neighborhoods retain estimates closer to the unpooled GLM values.

### Method 3: Bayesian Effect Encoding

For the most principled uncertainty quantification, use `step_lencode_bayes()`:
```{r bayes-encoding, eval=FALSE}
# Requires rstanarm package
ames_bayes <-
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>%
  step_lencode_bayes(Neighborhood, outcome = vars(Sale_Price)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_")) %>%
  step_ns(Latitude, Longitude, deg_free = 20)
```

## Visualizing Neighborhood Effects

```{r neighborhood-viz, fig.cap="Mean sale price by neighborhood with 90% confidence intervals"}
ames_train %>%
  group_by(Neighborhood) %>%
  summarize(
    mean = mean(Sale_Price),
    std_err = sd(Sale_Price) / sqrt(n()),
    .groups = "drop"
  ) %>%
  ggplot(aes(y = reorder(Neighborhood, mean), x = mean)) +
  geom_point() +
  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err)) +
  labs(
    y = NULL,
    x = "Sale Price (mean with 90% CI)",
    title = "Neighborhood Effects on Sale Price"
  ) +
  theme_minimal()
```

## Building a Complete Model

Here's how to use effect encoding in a complete modeling workflow.
This example requires the `parsnip`, `workflows`, and `yardstick` packages:

```{r workflow, eval=FALSE}
library(parsnip)
library(workflows)
library(yardstick)

# Define the recipe with mixed effect encoding
ames_rec <-
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>%
  step_lencode_mixed(Neighborhood, outcome = vars(Sale_Price)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_")) %>%
  step_ns(Latitude, Longitude, deg_free = 20)

# Define a linear regression model
lm_spec <- linear_reg() %>%
  set_engine("lm")

# Create and fit the workflow
ames_wf <- workflow() %>%
  add_recipe(ames_rec) %>%
  add_model(lm_spec)

ames_fit <- fit(ames_wf, data = ames_train)

# Evaluate on test set
ames_pred <- predict(ames_fit, ames_test) %>%
  bind_cols(ames_test %>% select(Sale_Price))

ames_pred %>%
  metrics(truth = Sale_Price, estimate = .pred)
```

## Additional Encoding Methods in embedmit

Beyond effect encoding, `embedmit` provides other useful transformations:

- `step_woe()` - Weight of Evidence encoding
- `step_collapse_cart()` - Collapse factor levels using CART
- `step_collapse_stringdist()` - Collapse similar levels based on string distance
- `step_discretize_cart()` - Discretize numeric variables using CART
- `step_discretize_xgb()` - Discretize using XGBoost
- `step_umap()` - UMAP embeddings (using uwotlite)

## Summary

Effect encoding is a powerful technique for handling high-cardinality categorical
variables. The `embedmit` package provides three approaches:

| Method | Function | Characteristics |
|--------|----------|-----------------|
| GLM | `step_lencode_glm()` | Fast, no pooling, may overfit rare categories |
| Mixed | `step_lencode_mixed()` | Partial pooling, better for small samples |
| Bayesian | `step_lencode_bayes()` | Full uncertainty quantification |

All methods gracefully handle novel categories at prediction time by using a
learned default encoding.
