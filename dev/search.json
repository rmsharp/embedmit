[{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/CLAUDE.html","id":"project-overview","dir":"","previous_headings":"","what":"Project Overview","title":"embedmit - Claude Code Project Configuration","text":"embedmit MIT-licensed fork embed package, providing recipe steps embedding categorical predictors dimensionality reduction tidymodels ecosystem.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CLAUDE.html","id":"mandatory-claude-code-testing-requirements","dir":"","previous_headings":"","what":"MANDATORY: Claude Code Testing Requirements","title":"embedmit - Claude Code Project Configuration","text":"SECTION NON-NEGOTIABLE. Claude MUST follow rules.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CLAUDE.html","id":"pre-commit-testing-is-required","dir":"","previous_headings":"MANDATORY: Claude Code Testing Requirements","what":"Pre-Commit Testing is REQUIRED","title":"embedmit - Claude Code Project Configuration","text":"commit, Claude MUST run full test suite: tests fail, Claude MUST fix committing. EXCEPTIONS.","code":"make test-all"},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/CLAUDE.html","id":"bug-fixes-must-include-regression-tests","dir":"","previous_headings":"MANDATORY: Claude Code Testing Requirements","what":"Bug Fixes MUST Include Regression Tests","title":"embedmit - Claude Code Project Configuration","text":"Every bug fix MUST include test : 1. failed fix 2. Passes fix 3. Prevents bug recurring","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CLAUDE.html","id":"how-it-works","dir":"","previous_headings":"MANDATORY: Claude Code Testing Requirements","what":"How It Works","title":"embedmit - Claude Code Project Configuration","text":"Claude asked commit: 1. Claude MUST run make test-first 2. tests fail, Claude fixes committing 3. git hook provides safety net Claude forgets developing new features: 1. Claude creates tests FIRST (TDD) 2. Claude implements feature 3. Claude verifies tests pass marking work complete manually verify tests:","code":"make test-all      # Full test suite (lint + unit + comparison) make precommit     # Same as above with detailed output"},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/CLAUDE.html","id":"testing-commands","dir":"","previous_headings":"Development Standards","what":"Testing Commands","title":"embedmit - Claude Code Project Configuration","text":"","code":"# Makefile targets (PREFERRED) make test          # Run unit tests only make test-all      # Run lint + unit tests + comparison tests make check         # Full R CMD check make precommit     # Pre-commit test suite with clear output  # R commands (alternative) Rscript -e \"devtools::test()\" Rscript -e \"devtools::check()\" Rscript -e \"testthat::test_file('tests/testthat/test-zzz_comparison_embed.R')\""},{"path":"https://rmsharp.github.io/embedmit/dev/CLAUDE.html","id":"pre-commit-checklist","dir":"","previous_headings":"Development Standards","what":"Pre-Commit Checklist","title":"embedmit - Claude Code Project Configuration","text":"committing code changes: Run make test-- tests must pass Run make check - errors warnings New features corresponding tests Bug fixes include regression tests Comparison tests pass (embed package available)","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CLAUDE.html","id":"code-style","dir":"","previous_headings":"Development Standards","what":"Code Style","title":"embedmit - Claude Code Project Configuration","text":"Follow tidyverse style guide Use roxygen2 documentation Export S3 methods properly recipes integration Maintain API compatibility original embed package","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CLAUDE.html","id":"key-files","dir":"","previous_headings":"Development Standards","what":"Key Files","title":"embedmit - Claude Code Project Configuration","text":"R/ - Source code recipe steps tests/testthat/ - Test files tests/testthat/helper_comparison.R - Utilities embed comparison tests/testthat/test-zzz_comparison_embed.R - Comparison test suite Makefile - Build test automation .githooks/pre-commit - Git pre-commit hook","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CLAUDE.html","id":"comparison-test-standards","dir":"","previous_headings":"Development Standards","what":"Comparison Test Standards","title":"embedmit - Claude Code Project Configuration","text":"embed package available, comparison tests verify: - Deterministic functions produce exact matches (tolerance 1e-10) - UMAP quality metrics: trustworthiness difference < 0.1 - implementations achieve > 0.85 trustworthiness - Performance within 2x original","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CLAUDE.html","id":"git-hook-setup","dir":"","previous_headings":"","what":"Git Hook Setup","title":"embedmit - Claude Code Project Configuration","text":"enable automatic pre-commit testing: ensures make precommit runs every commit. Can bypassed git commit ---verify emergencies .","code":"git config core.hooksPath .githooks"},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement codeofconduct@posit.co. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to embed","title":"Contributing to embed","text":"outlines propose change embed. detailed info contributing , tidyverse packages, please see development contributing guide.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to embed","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to embed","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed).","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to embed","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"topepo/embed\", fork = TRUE). Install development dependences devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to embed","text":"New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to embed","text":"Please note embed project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 embed authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/articles/Applications/GLM.html","id":"no-pooling","dir":"Articles > Applications","previous_headings":"","what":"No Pooling","title":"Using Generalized Linear Models","text":"case, effect sponsor code can estimated separately factor level. One method conducting estimation step fit logistic regression acceptance classification outcome sponsor code predictor. , log-odds naturally estimated logistic regression. data, recipe created step_lencode_glm used: tidy method can used extract encodings merged raw estimates: sponsor codes n > 1, estimates effectively : Note also effect used novel sponsor code future data sets average effect:","code":"grants_glm <-   recipe(class ~ ., data = grants_other) |>   # specify the variable being encoded and the outcome   step_lencode_glm(sponsor_code, outcome = vars(class)) |>   # estimate the effects   prep(training = grants_other) glm_estimates <-   tidy(grants_glm, number = 1) |>   dplyr::select(-terms, -id) glm_estimates ## # A tibble: 292 × 2 ##    level     value ##    <chr>     <dbl> ##  1 100D   4.05e- 1 ##  2 101A  -1.95e+ 0 ##  3 103C  -2.08e+ 0 ##  4 105A  -1.61e+ 0 ##  5 107C   1.66e+ 1 ##  6 10B    1.66e+ 1 ##  7 111C  -1.61e+ 0 ##  8 112D   6.93e- 1 ##  9 113A   0        ## 10 118B   3.14e-16 ## # ℹ 282 more rows glm_estimates <-   glm_estimates |>   set_names(c(\"sponsor_code\", \"glm\")) |>   inner_join(props, by = \"sponsor_code\") glm_estimates |>   dplyr::filter(is.finite(log_odds)) |>   mutate(difference = log_odds - glm) |>   dplyr::select(difference) |>   summary() ##    difference         ##  Min.   :-1.776e-15   ##  1st Qu.:-2.220e-16   ##  Median : 0.000e+00   ##  Mean   :-2.789e-17   ##  3rd Qu.: 2.220e-16   ##  Max.   : 8.882e-16 tidy(grants_glm, number = 1) |>   dplyr::filter(level == \"..new\") |>   select(-id) ## # A tibble: 1 × 3 ##   level value terms        ##   <chr> <dbl> <chr>        ## 1 ..new -2.88 sponsor_code"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/Applications/GLM.html","id":"partial-pooling","dir":"Articles > Applications","previous_headings":"","what":"Partial Pooling","title":"Using Generalized Linear Models","text":"method estimates effects using sponsor codes using hierarchical Bayesian generalized linear model. sponsor codes treated random set contributes random intercept previously used logistic regression. Partial pooling estimates effect combination separate empirical estimates log-odds prior distribution. sponsor codes small sample sizes, final estimate shrunken towards overall mean log-odds. makes sense since poor information estimating sponsor codes. sponsor codes many data points, estimates reply empirical estimates. page good discussion pooling using Bayesian models.","code":"# due to Matrix problems knitr::knit_exit()"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Encoding Categorical Data","text":"statistical modeling R, preferred representation categorical nominal data factor, variable can take limited number different values. Internally, factors stored vector integer values together set text labels. straightforward approach transforming categorical variable numeric representation create dummy indicator variables levels. However, approach work well variable high cardinality (many levels) may encounter novel values prediction time (new levels). vignette explores alternative encoding strategies address challenges.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Encoding Categorical Data","text":"","code":"library(embedmit) library(recipes) library(dplyr) library(rsample) library(ggplot2) library(purrr) library(modeldata)  # Load the Ames housing data data(ames)  # Create train/test split set.seed(502) ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price) ames_train <- training(ames_split) ames_test <- testing(ames_split)"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"is-an-encoding-necessary","dir":"Articles","previous_headings":"","what":"Is an Encoding Necessary?","title":"Encoding Categorical Data","text":"minority models, based trees rules, can handle categorical data natively require encoding transformation kinds features. example: Tree-based models (decision trees, random forests, boosted trees) can find optimal splits categorical variables directly Naive Bayes models compute class probabilities without requiring numeric encoding models, research shown creating dummy variables typically improve performance can increase computation time. recipes package provides step_dummy() standard dummy encoding needed.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"using-the-outcome-for-encoding-predictors","dir":"Articles","previous_headings":"","what":"Using the Outcome for Encoding Predictors","title":"Encoding Categorical Data","text":"multiple options encodings complex dummy indicator variables. One method called effect likelihood encodings replaces original categorical variables single numeric column measures effect data. example, Ames housing data, can compute mean median sale price neighborhood use value represent categorical level. Effect encodings can also seamlessly handle situations novel factor level encountered data.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"visualizing-neighborhood-effects","dir":"Articles","previous_headings":"Using the Outcome for Encoding Predictors","what":"Visualizing Neighborhood Effects","title":"Encoding Categorical Data","text":"Ames data 28 different neighborhoods. Let’s visualize sale price varies across : Mean sale price neighborhood 90% confidence intervals. Neighborhoods ordered mean sale price.","code":"ames_train %>%   group_by(Neighborhood) %>%   summarize(     mean = mean(Sale_Price),     std_err = sd(Sale_Price) / sqrt(length(Sale_Price)),     .groups = \"drop\"   ) %>%   ggplot(aes(y = reorder(Neighborhood, mean), x = mean)) +   geom_point() +   geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err)) +   labs(y = NULL, x = \"Price (mean, log scale)\") +   theme_minimal()"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"glm-based-effect-encoding","dir":"Articles","previous_headings":"Using the Outcome for Encoding Predictors","what":"GLM-Based Effect Encoding","title":"Encoding Categorical Data","text":"step_lencode_glm() function uses generalized linear model estimate effect category level: can examine learned encodings preparing recipe using tidy(): neighborhood replaced single numeric value representing effect sale price.","code":"ames_glm <- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +            Latitude + Longitude, data = ames_train) %>%   step_log(Gr_Liv_Area, base = 10) %>%   step_lencode_glm(Neighborhood, outcome = vars(Sale_Price)) %>%   step_dummy(all_nominal_predictors()) %>%   step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %>%   step_ns(Latitude, Longitude, deg_free = 20)  ames_glm glm_estimates <-   prep(ames_glm) %>%   tidy(number = 2)  glm_estimates #> # A tibble: 29 × 4 #>    level                value terms        id                #>    <chr>                <dbl> <chr>        <chr>             #>  1 North_Ames         144416. Neighborhood lencode_glm_yj20u #>  2 College_Creek      202763. Neighborhood lencode_glm_yj20u #>  3 Old_Town           124999. Neighborhood lencode_glm_yj20u #>  4 Edwards            130460. Neighborhood lencode_glm_yj20u #>  5 Somerset           232310. Neighborhood lencode_glm_yj20u #>  6 Northridge_Heights 321119. Neighborhood lencode_glm_yj20u #>  7 Gilbert            191159. Neighborhood lencode_glm_yj20u #>  8 Sawyer             137208. Neighborhood lencode_glm_yj20u #>  9 Northwest_Ames     188726. Neighborhood lencode_glm_yj20u #> 10 Sawyer_West        182651. Neighborhood lencode_glm_yj20u #> # ℹ 19 more rows"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"handling-novel-categories","dir":"Articles","previous_headings":"Using the Outcome for Encoding Predictors","what":"Handling Novel Categories","title":"Encoding Categorical Data","text":"Effect encodings can seamlessly handle situations novel factor level encountered data. encoding includes special ..new level: model encounters unseen neighborhood prediction time, uses default encoding.","code":"glm_estimates %>%   filter(level == \"..new\") #> # A tibble: 1 × 4 #>   level   value terms        id                #>   <chr>   <dbl> <chr>        <chr>             #> 1 ..new 183150. Neighborhood lencode_glm_yj20u"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"effect-encodings-with-partial-pooling","dir":"Articles","previous_headings":"","what":"Effect Encodings with Partial Pooling","title":"Encoding Categorical Data","text":"Creating effect encoding step_lencode_glm() estimates effect separately factor level (case, neighborhood). However, neighborhoods many houses , . much uncertainty measurement price neighborhoods training set homes neighborhoods many. can use partial pooling adjust estimates levels small sample sizes shrunk toward overall mean. step_lencode_mixed() function uses hierarchical mixed effects models accomplish :","code":"ames_mixed <-   recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +            Latitude + Longitude, data = ames_train) %>%   step_log(Gr_Liv_Area, base = 10) %>%   step_lencode_mixed(Neighborhood, outcome = vars(Sale_Price)) %>%   step_dummy(all_nominal_predictors()) %>%   step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %>%   step_ns(Latitude, Longitude, deg_free = 20)  ames_mixed mixed_estimates <-   prep(ames_mixed) %>%   tidy(number = 2)  mixed_estimates #> # A tibble: 29 × 4 #>    level                value terms        id                  #>    <chr>                <dbl> <chr>        <chr>               #>  1 North_Ames         144488. Neighborhood lencode_mixed_AmBz1 #>  2 College_Creek      202724. Neighborhood lencode_mixed_AmBz1 #>  3 Old_Town           125183. Neighborhood lencode_mixed_AmBz1 #>  4 Edwards            130698. Neighborhood lencode_mixed_AmBz1 #>  5 Somerset           232135. Neighborhood lencode_mixed_AmBz1 #>  6 Northridge_Heights 320533. Neighborhood lencode_mixed_AmBz1 #>  7 Gilbert            191145. Neighborhood lencode_mixed_AmBz1 #>  8 Sawyer             137433. Neighborhood lencode_mixed_AmBz1 #>  9 Northwest_Ames     188722. Neighborhood lencode_mixed_AmBz1 #> 10 Sawyer_West        182683. Neighborhood lencode_mixed_AmBz1 #> # ℹ 19 more rows mixed_estimates %>%   filter(level == \"..new\") #> # A tibble: 1 × 4 #>   level   value terms        id                  #>   <chr>   <dbl> <chr>        <chr>               #> 1 ..new 183225. Neighborhood lencode_mixed_AmBz1"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"comparing-pooling-methods","dir":"Articles","previous_headings":"Effect Encodings with Partial Pooling","what":"Comparing Pooling Methods","title":"Encoding Categorical Data","text":"Let’s visualize partial pooling affects estimates: Comparison GLM (pooling) versus mixed effects (partial pooling) encodings. Point size represents number observations neighborhood. use partial pooling, shrink effect estimates toward mean don’t much evidence price neighborhoods observations. Neighborhoods many observations retain estimates close unpooled GLM values.","code":"glm_estimates %>%   rename(`no pooling` = value) %>%   left_join(     mixed_estimates %>%       rename(`partial pooling` = value),     by = \"level\"   ) %>%   left_join(     ames_train %>%       count(Neighborhood) %>%       mutate(level = as.character(Neighborhood)),     by = \"level\"   ) %>%   filter(!is.na(n)) %>%   ggplot(aes(`no pooling`, `partial pooling`, size = sqrt(n))) +   geom_abline(color = \"gray50\", lty = 2) +   geom_point(alpha = 0.7) +   coord_fixed() +   labs(size = \"sqrt(n)\") +   theme_minimal()"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"bayesian-effect-encoding","dir":"Articles","previous_headings":"Effect Encodings with Partial Pooling","what":"Bayesian Effect Encoding","title":"Encoding Categorical Data","text":"fully Bayesian uncertainty quantification, step_lencode_bayes() provides alternative approach (requires rstanarm package):","code":"ames_bayes <-   recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +            Latitude + Longitude, data = ames_train) %>%   step_log(Gr_Liv_Area, base = 10) %>%   step_lencode_bayes(Neighborhood, outcome = vars(Sale_Price)) %>%   step_dummy(all_nominal_predictors()) %>%   step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %>%   step_ns(Latitude, Longitude, deg_free = 20)"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"feature-hashing","dir":"Articles","previous_headings":"","what":"Feature Hashing","title":"Encoding Categorical Data","text":"Feature hashing methods also create dummy variables, consider value category assign predefined pool dummy variables. hashing function takes input variable size maps output fixed size. feature hashing, number possible hashes hyperparameter set model developer computing modulo integer hashes: Feature hashing can handle new category levels prediction time, since rely pre-determined dummy variables. textrecipes package provides step_dummy_hash() approach.","code":"library(rlang)  ames_hashed <-   ames_train %>%   mutate(Hash = map_chr(Neighborhood, hash))  ames_hashed %>%   select(Neighborhood, Hash) %>%   head(6) #> # A tibble: 6 × 2 #>   Neighborhood    Hash                             #>   <fct>           <chr>                            #> 1 North_Ames      076543f71313e522efe157944169d919 #> 2 North_Ames      076543f71313e522efe157944169d919 #> 3 Briardale       b598bec306983e3e68a3118952df8cf0 #> 4 Briardale       b598bec306983e3e68a3118952df8cf0 #> 5 Northpark_Villa 6af95b5db968bf393e78188a81e0e1e4 #> 6 Northpark_Villa 6af95b5db968bf393e78188a81e0e1e4 ames_hashed %>%   mutate(     Hash = strtoi(substr(Hash, 26, 32), base = 16L),     Hash = Hash %% 16   ) %>%   select(Neighborhood, Hash) %>%   head(10) #> # A tibble: 10 × 2 #>    Neighborhood     Hash #>    <fct>           <dbl> #>  1 North_Ames          9 #>  2 North_Ames          9 #>  3 Briardale           0 #>  4 Briardale           0 #>  5 Northpark_Villa     4 #>  6 Northpark_Villa     4 #>  7 Sawyer_West         9 #>  8 Sawyer_West         9 #>  9 Sawyer              8 #> 10 Sawyer              8"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"more-encoding-options","dir":"Articles","previous_headings":"","what":"More Encoding Options","title":"Encoding Categorical Data","text":"embedmit package offers additional encoding methods:","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"chapter-summary","dir":"Articles","previous_headings":"","what":"Chapter Summary","title":"Encoding Categorical Data","text":"straightforward option transforming categorical variable numeric representation create dummy variables levels, option work well variable high cardinality (many levels) may see novel values prediction time (new levels). Effect encodings feature hashing address challenges: Effect encodings replace categories single numeric value measuring outcome relationship Partial pooling (via step_lencode_mixed()) adjusts estimates levels small sample sizes shrunk toward overall mean Feature hashing maps categories predefined pool dummy variables can handle novel categories options include entity embeddings (learned via neural network step_embed()) weight evidence transformation (via step_woe()).","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding-01.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Encoding Categorical Data","text":"Kuhn, M., & Johnson, K. (2019). Feature Engineering Selection. CRC Press. Kuhn, M., & Silge, J. (2022). Tidy Modeling R. O’Reilly Media. Micci-Barreca, D. (2001). preprocessing scheme high-cardinality categorical attributes classification prediction problems. ACM SIGKDD Explorations, 3(1), 27-32.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Encoding Categorical Data","text":"statistical modeling R, preferred representation categorical nominal data factor, variable can take limited number different values. Internally, factors stored vector integer values together set text labels. straightforward approach transforming categorical variable numeric representation create dummy indicator variables levels. However, approach work well variable high cardinality (many levels) may encounter novel values prediction time (new levels). vignette explores alternative encoding strategies address challenges.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Encoding Categorical Data","text":"","code":"library(embedmit) library(recipes) library(dplyr) library(tidyr) library(rsample) library(ggplot2) library(purrr) library(modeldata) library(knitr)  # Load the Ames housing data data(ames)  # Create train/test split set.seed(502) ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price) ames_train <- training(ames_split) ames_test <- testing(ames_split)"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"is-an-encoding-necessary","dir":"Articles","previous_headings":"","what":"Is an Encoding Necessary?","title":"Encoding Categorical Data","text":"minority models, based trees rules, can handle categorical data natively require encoding transformation kinds features. example: - Tree-based models (decision trees, random forests, boosted trees) can find optimal splits categorical variables directly - Naive Bayes models compute class probabilities without requiring numeric encoding - Rule-based models can directly use categorical conditions models, research shown creating dummy variables typically improve performance can increase computation time. Starting untransformed categorical variables model allows often best approach. However, majority models require numeric representations predictors. cases, need encoding strategies.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"dummy-variables","dir":"Articles","previous_headings":"","what":"Dummy Variables","title":"Encoding Categorical Data","text":"common approach create dummy indicator variables levels categorical variable. Table 17.1 shows Bldg_Type variable Ames data (five categories) can encoded using four dummy variables: Table 17.1: Dummy variable encodings building type categories. first factor level (OneFam) becomes reference level encoded zeros. One disadvantage approach reference level absorbed intercept, making interpretation less intuitive. recipes package provides step_dummy() standard dummy encoding needed.","code":"bldg_types <- c(\"OneFam\", \"TwoFmCon\", \"Duplex\", \"Twnhs\", \"TwnhsE\")  dummy_df <- tibble(   `Bldg_Type` = bldg_types,   `TwoFmCon` = c(0, 1, 0, 0, 0),   `Duplex` = c(0, 0, 1, 0, 0),   `Twnhs` = c(0, 0, 0, 1, 0),   `TwnhsE` = c(0, 0, 0, 0, 1) )  kable(dummy_df, align = \"lcccc\")"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"encoding-ordinal-predictors","dir":"Articles","previous_headings":"","what":"Encoding Ordinal Predictors","title":"Encoding Categorical Data","text":"Sometimes categorical predictors ordinal, meaning natural ordering levels. example, “none,” “little,” “,” “bunch,” “copious amounts” clear progression. ordinal variables, standard dummy encoding ignores ordering information. recipes package offers polynomial expansions preserve ordinality. Table 17.2 shows linear, quadratic, cubic, quartic polynomial expansions: Table 17.2: Polynomial expansions ordinal predictors. linear column captures main trend, quadratic captures curvature, . Consider using step_unorder() convert ordered factors regular factors, step_ordinalscore() convert numeric scores based rank.","code":"ordinal_levels <- c(\"none\", \"a little\", \"some\", \"a bunch\", \"copious amounts\")  ordinal_df <- tibble(   Level = ordinal_levels,   Linear = c(-0.632, -0.316, 0.000, 0.316, 0.632),   Quadratic = c(0.535, -0.267, -0.535, -0.267, 0.535),   Cubic = c(-0.316, 0.632, 0.000, -0.632, 0.316),   Quartic = c(0.119, -0.478, 0.717, -0.478, 0.119) )  kable(ordinal_df, digits = 3, align = \"lcccc\")"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"using-the-outcome-for-encoding-predictors","dir":"Articles","previous_headings":"","what":"Using the Outcome for Encoding Predictors","title":"Encoding Categorical Data","text":"multiple options encodings complex dummy indicator variables. One method called effect likelihood encodings replaces original categorical variables single numeric column measures effect data. example, Ames housing data, can compute mean median sale price neighborhood use value represent categorical level. Effect encodings can also seamlessly handle situations novel factor level encountered data. Ames data 28 different neighborhoods. Let’s visualize sale price varies across :  Mean sale price neighborhood 90% confidence intervals. Neighborhoods ordered mean sale price. step_lencode_glm() function uses generalized linear model estimate effect category level: can examine learned encodings preparing recipe using tidy(): neighborhood replaced single numeric value representing effect sale price. Effect encodings can seamlessly handle situations novel factor level encountered data. encoding includes special ..new level: model encounters unseen neighborhood prediction time, uses default encoding. Important: Overfitting Risk Effect Encodings create effect encoding categorical variable, effectively layering mini-model inside actual model. supervised preprocessing must rigorously resampled avoid overfitting. effect encoding estimated separately within resample, just like model parameters. Using tidymodels framework workflow() ensures happens automatically.","code":"ames_train %>%   group_by(Neighborhood) %>%   summarize(     mean = mean(Sale_Price),     std_err = sd(Sale_Price) / sqrt(length(Sale_Price)),     .groups = \"drop\"   ) %>%   ggplot(aes(y = reorder(Neighborhood, mean), x = mean)) +   geom_point() +   geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err)) +   labs(y = NULL, x = \"Price (mean, log scale)\") +   theme_minimal() ames_glm <- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +            Latitude + Longitude, data = ames_train) %>%   step_log(Gr_Liv_Area, base = 10) %>%   step_lencode_glm(Neighborhood, outcome = vars(Sale_Price)) %>%   step_dummy(all_nominal_predictors()) %>%   step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %>%   step_ns(Latitude, Longitude, deg_free = 20)  ames_glm glm_estimates <-   prep(ames_glm) %>%   tidy(number = 2)  glm_estimates #> # A tibble: 29 × 4 #>    level                value terms        id                #>    <chr>                <dbl> <chr>        <chr>             #>  1 North_Ames         144416. Neighborhood lencode_glm_yj20u #>  2 College_Creek      202763. Neighborhood lencode_glm_yj20u #>  3 Old_Town           124999. Neighborhood lencode_glm_yj20u #>  4 Edwards            130460. Neighborhood lencode_glm_yj20u #>  5 Somerset           232310. Neighborhood lencode_glm_yj20u #>  6 Northridge_Heights 321119. Neighborhood lencode_glm_yj20u #>  7 Gilbert            191159. Neighborhood lencode_glm_yj20u #>  8 Sawyer             137208. Neighborhood lencode_glm_yj20u #>  9 Northwest_Ames     188726. Neighborhood lencode_glm_yj20u #> 10 Sawyer_West        182651. Neighborhood lencode_glm_yj20u #> # ℹ 19 more rows glm_estimates %>%   filter(level == \"..new\") #> # A tibble: 1 × 4 #>   level   value terms        id                #>   <chr>   <dbl> <chr>        <chr>             #> 1 ..new 183150. Neighborhood lencode_glm_yj20u"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"visualizing-neighborhood-effects","dir":"Articles","previous_headings":"","what":"Visualizing Neighborhood Effects","title":"Encoding Categorical Data","text":"Ames data 28 different neighborhoods. Let’s visualize sale price varies across :  Mean sale price neighborhood 90% confidence intervals. Neighborhoods ordered mean sale price.","code":"ames_train %>%   group_by(Neighborhood) %>%   summarize(     mean = mean(Sale_Price),     std_err = sd(Sale_Price) / sqrt(length(Sale_Price)),     .groups = \"drop\"   ) %>%   ggplot(aes(y = reorder(Neighborhood, mean), x = mean)) +   geom_point() +   geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err)) +   labs(y = NULL, x = \"Price (mean, log scale)\") +   theme_minimal()"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"glm-based-effect-encoding","dir":"Articles","previous_headings":"","what":"GLM-Based Effect Encoding","title":"Encoding Categorical Data","text":"step_lencode_glm() function uses generalized linear model estimate effect category level: can examine learned encodings preparing recipe using tidy(): neighborhood replaced single numeric value representing effect sale price.","code":"ames_glm <- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +            Latitude + Longitude, data = ames_train) %>%   step_log(Gr_Liv_Area, base = 10) %>%   step_lencode_glm(Neighborhood, outcome = vars(Sale_Price)) %>%   step_dummy(all_nominal_predictors()) %>%   step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %>%   step_ns(Latitude, Longitude, deg_free = 20)  ames_glm glm_estimates <-   prep(ames_glm) %>%   tidy(number = 2)  glm_estimates #> # A tibble: 29 × 4 #>    level                value terms        id                #>    <chr>                <dbl> <chr>        <chr>             #>  1 North_Ames         144416. Neighborhood lencode_glm_yj20u #>  2 College_Creek      202763. Neighborhood lencode_glm_yj20u #>  3 Old_Town           124999. Neighborhood lencode_glm_yj20u #>  4 Edwards            130460. Neighborhood lencode_glm_yj20u #>  5 Somerset           232310. Neighborhood lencode_glm_yj20u #>  6 Northridge_Heights 321119. Neighborhood lencode_glm_yj20u #>  7 Gilbert            191159. Neighborhood lencode_glm_yj20u #>  8 Sawyer             137208. Neighborhood lencode_glm_yj20u #>  9 Northwest_Ames     188726. Neighborhood lencode_glm_yj20u #> 10 Sawyer_West        182651. Neighborhood lencode_glm_yj20u #> # ℹ 19 more rows"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"handling-novel-categories","dir":"Articles","previous_headings":"","what":"Handling Novel Categories","title":"Encoding Categorical Data","text":"Effect encodings can seamlessly handle situations novel factor level encountered data. encoding includes special ..new level: model encounters unseen neighborhood prediction time, uses default encoding. Important: Overfitting Risk Effect Encodings create effect encoding categorical variable, effectively layering mini-model inside actual model. supervised preprocessing must rigorously resampled avoid overfitting. effect encoding estimated separately within resample, just like model parameters. Using tidymodels framework workflow() ensures happens automatically.","code":"glm_estimates %>%   filter(level == \"..new\") #> # A tibble: 1 × 4 #>   level   value terms        id                #>   <chr>   <dbl> <chr>        <chr>             #> 1 ..new 183150. Neighborhood lencode_glm_yj20u"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"effect-encodings-with-partial-pooling","dir":"Articles","previous_headings":"","what":"Effect Encodings with Partial Pooling","title":"Encoding Categorical Data","text":"Creating effect encoding step_lencode_glm() estimates effect separately factor level (case, neighborhood). However, neighborhoods many houses , . much uncertainty measurement price neighborhoods training set homes neighborhoods many. can use partial pooling adjust estimates levels small sample sizes shrunk toward overall mean. step_lencode_mixed() function uses hierarchical mixed effects models accomplish : Let’s visualize partial pooling affects estimates:  Comparison GLM (pooling) versus mixed effects (partial pooling) encodings. Point size represents number observations neighborhood. use partial pooling, shrink effect estimates toward mean don’t much evidence price neighborhoods observations. Neighborhoods many observations retain estimates close unpooled GLM values. fully Bayesian uncertainty quantification, step_lencode_bayes() provides alternative approach (requires rstanarm package): approach provides full posterior distributions effect estimate, allowing uncertainty quantification downstream analyses.","code":"ames_mixed <-   recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +            Latitude + Longitude, data = ames_train) %>%   step_log(Gr_Liv_Area, base = 10) %>%   step_lencode_mixed(Neighborhood, outcome = vars(Sale_Price)) %>%   step_dummy(all_nominal_predictors()) %>%   step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %>%   step_ns(Latitude, Longitude, deg_free = 20)  ames_mixed mixed_estimates <-   prep(ames_mixed) %>%   tidy(number = 2)  mixed_estimates #> # A tibble: 29 × 4 #>    level                value terms        id                  #>    <chr>                <dbl> <chr>        <chr>               #>  1 North_Ames         144488. Neighborhood lencode_mixed_AmBz1 #>  2 College_Creek      202724. Neighborhood lencode_mixed_AmBz1 #>  3 Old_Town           125183. Neighborhood lencode_mixed_AmBz1 #>  4 Edwards            130698. Neighborhood lencode_mixed_AmBz1 #>  5 Somerset           232135. Neighborhood lencode_mixed_AmBz1 #>  6 Northridge_Heights 320533. Neighborhood lencode_mixed_AmBz1 #>  7 Gilbert            191145. Neighborhood lencode_mixed_AmBz1 #>  8 Sawyer             137433. Neighborhood lencode_mixed_AmBz1 #>  9 Northwest_Ames     188722. Neighborhood lencode_mixed_AmBz1 #> 10 Sawyer_West        182683. Neighborhood lencode_mixed_AmBz1 #> # ℹ 19 more rows mixed_estimates %>%   filter(level == \"..new\") #> # A tibble: 1 × 4 #>   level   value terms        id                  #>   <chr>   <dbl> <chr>        <chr>               #> 1 ..new 183225. Neighborhood lencode_mixed_AmBz1 glm_estimates %>%   rename(`no pooling` = value) %>%   left_join(     mixed_estimates %>%       rename(`partial pooling` = value),     by = \"level\"   ) %>%   left_join(     ames_train %>%       count(Neighborhood) %>%       mutate(level = as.character(Neighborhood)),     by = \"level\"   ) %>%   filter(!is.na(n)) %>%   ggplot(aes(`no pooling`, `partial pooling`, size = sqrt(n))) +   geom_abline(color = \"gray50\", lty = 2) +   geom_point(alpha = 0.7) +   coord_fixed() +   labs(size = \"sqrt(n)\") +   theme_minimal() ames_bayes <-   recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +            Latitude + Longitude, data = ames_train) %>%   step_log(Gr_Liv_Area, base = 10) %>%   step_lencode_bayes(Neighborhood, outcome = vars(Sale_Price)) %>%   step_dummy(all_nominal_predictors()) %>%   step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %>%   step_ns(Latitude, Longitude, deg_free = 20)"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"comparing-pooling-methods","dir":"Articles","previous_headings":"","what":"Comparing Pooling Methods","title":"Encoding Categorical Data","text":"Let’s visualize partial pooling affects estimates:  Comparison GLM (pooling) versus mixed effects (partial pooling) encodings. Point size represents number observations neighborhood. use partial pooling, shrink effect estimates toward mean don’t much evidence price neighborhoods observations. Neighborhoods many observations retain estimates close unpooled GLM values.","code":"glm_estimates %>%   rename(`no pooling` = value) %>%   left_join(     mixed_estimates %>%       rename(`partial pooling` = value),     by = \"level\"   ) %>%   left_join(     ames_train %>%       count(Neighborhood) %>%       mutate(level = as.character(Neighborhood)),     by = \"level\"   ) %>%   filter(!is.na(n)) %>%   ggplot(aes(`no pooling`, `partial pooling`, size = sqrt(n))) +   geom_abline(color = \"gray50\", lty = 2) +   geom_point(alpha = 0.7) +   coord_fixed() +   labs(size = \"sqrt(n)\") +   theme_minimal()"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"bayesian-effect-encoding","dir":"Articles","previous_headings":"","what":"Bayesian Effect Encoding","title":"Encoding Categorical Data","text":"fully Bayesian uncertainty quantification, step_lencode_bayes() provides alternative approach (requires rstanarm package): approach provides full posterior distributions effect estimate, allowing uncertainty quantification downstream analyses.","code":"ames_bayes <-   recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +            Latitude + Longitude, data = ames_train) %>%   step_log(Gr_Liv_Area, base = 10) %>%   step_lencode_bayes(Neighborhood, outcome = vars(Sale_Price)) %>%   step_dummy(all_nominal_predictors()) %>%   step_interact(~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) %>%   step_ns(Latitude, Longitude, deg_free = 20)"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"feature-hashing","dir":"Articles","previous_headings":"","what":"Feature Hashing","title":"Encoding Categorical Data","text":"Feature hashing methods also create dummy variables, consider value category assign predefined pool dummy variables. hashing function takes input variable size maps output fixed size. Feature hashing several advantages: Memory efficient: number columns fixed regardless cardinality Handles novel categories: New levels automatically get hashed Fast: Hash computation efficient However, feature hashing directly interpretable hash functions reversed, multiple categories can map hash value (collisions). feature hashing, number possible hashes hyperparameter set model developer computing modulo integer hashes: multiple categories map hash value, called collision. Table 17.3 shows collision frequency hashing 28 Ames neighborhoods 16 buckets: Table 17.3: Hash collision frequency Ames neighborhoods 16 hash buckets. Using signed = TRUE feature hashing can reduce impact collisions assigning +1 -1 instead just 1. textrecipes package provides step_dummy_hash() approach. Choosing Number Hash Buckets number hash buckets tuning parameter. buckets mean fewer collisions columns. Consider using step_zv() hashing remove zero-variance columns.","code":"library(rlang)  ames_hashed <-   ames_train %>%   mutate(Hash = map_chr(Neighborhood, hash))  ames_hashed %>%   select(Neighborhood, Hash) %>%   head(6) #> # A tibble: 6 × 2 #>   Neighborhood    Hash                             #>   <fct>           <chr>                            #> 1 North_Ames      076543f71313e522efe157944169d919 #> 2 North_Ames      076543f71313e522efe157944169d919 #> 3 Briardale       b598bec306983e3e68a3118952df8cf0 #> 4 Briardale       b598bec306983e3e68a3118952df8cf0 #> 5 Northpark_Villa 6af95b5db968bf393e78188a81e0e1e4 #> 6 Northpark_Villa 6af95b5db968bf393e78188a81e0e1e4 ames_hashed %>%   mutate(     Hash = strtoi(substr(Hash, 26, 32), base = 16L),     Hash = Hash %% 16   ) %>%   select(Neighborhood, Hash) %>%   head(10) #> # A tibble: 10 × 2 #>    Neighborhood     Hash #>    <fct>           <dbl> #>  1 North_Ames          9 #>  2 North_Ames          9 #>  3 Briardale           0 #>  4 Briardale           0 #>  5 Northpark_Villa     4 #>  6 Northpark_Villa     4 #>  7 Sawyer_West         9 #>  8 Sawyer_West         9 #>  9 Sawyer              8 #> 10 Sawyer              8 collision_df <- ames_train %>%   mutate(     Hash = map_chr(Neighborhood, hash),     Hash = strtoi(substr(Hash, 26, 32), base = 16L),     Hash = Hash %% 16   ) %>%   distinct(Neighborhood, Hash) %>%   count(Hash, name = \"Neighborhoods\") %>%   arrange(Hash)  # Show summary collision_summary <- collision_df %>%   count(Neighborhoods, name = \"Hash_Buckets\") %>%   arrange(desc(Neighborhoods))  kable(collision_summary,       col.names = c(\"Neighborhoods per Bucket\", \"Number of Buckets\"),       align = \"cc\")"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"hash-collisions","dir":"Articles","previous_headings":"","what":"Hash Collisions","title":"Encoding Categorical Data","text":"multiple categories map hash value, called collision. Table 17.3 shows collision frequency hashing 28 Ames neighborhoods 16 buckets: Table 17.3: Hash collision frequency Ames neighborhoods 16 hash buckets. Using signed = TRUE feature hashing can reduce impact collisions assigning +1 -1 instead just 1. textrecipes package provides step_dummy_hash() approach. Choosing Number Hash Buckets number hash buckets tuning parameter. buckets mean fewer collisions columns. Consider using step_zv() hashing remove zero-variance columns.","code":"collision_df <- ames_train %>%   mutate(     Hash = map_chr(Neighborhood, hash),     Hash = strtoi(substr(Hash, 26, 32), base = 16L),     Hash = Hash %% 16   ) %>%   distinct(Neighborhood, Hash) %>%   count(Hash, name = \"Neighborhoods\") %>%   arrange(Hash)  # Show summary collision_summary <- collision_df %>%   count(Neighborhoods, name = \"Hash_Buckets\") %>%   arrange(desc(Neighborhoods))  kable(collision_summary,       col.names = c(\"Neighborhoods per Bucket\", \"Number of Buckets\"),       align = \"cc\")"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"entity-embeddings","dir":"Articles","previous_headings":"","what":"Entity Embeddings","title":"Encoding Categorical Data","text":"Entity embeddings use neural network learn lower-dimensional representation categorical variables. particularly useful high-cardinality variables dummy encoding create many columns. step_embed() function creates learned embeddings: Key hyperparameters entity embeddings include: Entity embeddings can capture complex nonlinear relationships categories outcome.","code":"# Requires keras/tensorflow ames_embed <-   recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +            Latitude + Longitude, data = ames_train) %>%   step_log(Gr_Liv_Area, base = 10) %>%   step_embed(Neighborhood, outcome = vars(Sale_Price),              num_terms = 5,        # Number of embedding dimensions              hidden_units = 10,    # Hidden layer size              options = embed_control(epochs = 20)) %>%   step_dummy(all_nominal_predictors()) %>%   step_ns(Latitude, Longitude, deg_free = 20)"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"weight-of-evidence","dir":"Articles","previous_headings":"","what":"Weight of Evidence","title":"Encoding Categorical Data","text":"Weight Evidence (WoE) transformation designed binary classification outcomes. encodes category level based logarithm ratio good/bad outcomes: WoEi=ln(Distribution EventsiDistribution Non-Eventsi) \\text{WoE}_i = \\ln\\left(\\frac{\\text{Distribution Events}_i}{\\text{Distribution Non-Events}_i}\\right) step_woe() function implements encoding: WoE particularly popular credit scoring risk modeling applications. Positive WoE values indicate categories associated higher event rates.","code":"# For binary outcome binary_recipe <-   recipe(outcome ~ category, data = binary_data) %>%   step_woe(category, outcome = vars(outcome))"},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"summary-of-encoding-options","dir":"Articles","previous_headings":"","what":"Summary of Encoding Options","title":"Encoding Categorical Data","text":"embedmit package offers multiple encoding strategies. choice depends specific situation:","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"chapter-summary","dir":"Articles","previous_headings":"","what":"Chapter Summary","title":"Encoding Categorical Data","text":"straightforward option transforming categorical variable numeric representation create dummy variables levels, option work well variable high cardinality (many levels) may see novel values prediction time (new levels). Key takeaways: Consider native handling first: Tree-based models Naive Bayes can handle categories directly Effect encodings replace categories single numeric value measuring outcome relationship Partial pooling (via step_lencode_mixed()) adjusts estimates levels small sample sizes shrunk toward overall mean Feature hashing maps categories predefined pool dummy variables can handle novel categories Entity embeddings learn complex representations via neural networks Always resample supervised encodings avoid overfitting","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/articles/categorical-encoding.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Encoding Categorical Data","text":"Kuhn, M., & Johnson, K. (2019). Feature Engineering Selection. CRC Press. Kuhn, M., & Silge, J. (2022). Tidy Modeling R. O’Reilly Media. Micci-Barreca, D. (2001). preprocessing scheme high-cardinality categorical attributes classification prediction problems. ACM SIGKDD Explorations, 3(1), 27-32. Guo, C., & Berkhahn, F. (2016). Entity embeddings categorical variables. arXiv preprint arXiv:1604.06737.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Emil Hvitfeldt. Author, maintainer. Max Kuhn. Author. Posit Software, PBC. Copyright holder, funder.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hvitfeldt E, Kuhn M (2026). embedmit: Extra Recipes Encoding Predictors (MIT-Compatible Fork). R package version 1.2.1.9000, https://github.com/rmsharp/embedmit.","code":"@Manual{,   title = {embedmit: Extra Recipes for Encoding Predictors (MIT-Compatible Fork)},   author = {Emil Hvitfeldt and Max Kuhn},   year = {2026},   note = {R package version 1.2.1.9000},   url = {https://github.com/rmsharp/embedmit}, }"},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/index.html","id":"mit-compatible-fork-of-embed","dir":"","previous_headings":"","what":"MIT-Compatible Fork of embed","title":"Extra Recipes for Encoding Predictors (MIT-Compatible Fork)","text":"embedmit MIT-licensed fork embed package tidymodels. provides recipe steps encoding predictors avoids AGPL-licensed dependencies runtime.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/index.html","id":"why-embedmit","dir":"","previous_headings":"MIT-Compatible Fork of embed","what":"Why embedmit?","title":"Extra Recipes for Encoding Predictors (MIT-Compatible Fork)","text":"original embed package depends uwot UMAP functionality, turn depends dqrng (AGPL-3 licensed). creates licensing complications projects need maintain MIT permissive licensing throughout dependency tree. embedmit solves : 1. Depending uwotmit instead uwot - MIT-licensed fork replaces dqrng sitmo 2. Using rng_type = \"tausworthe\" default additional safety","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/index.html","id":"technical-changes","dir":"","previous_headings":"MIT-Compatible Fork of embed","what":"Technical Changes","title":"Extra Recipes for Encoding Predictors (MIT-Compatible Fork)","text":"Dependency change: uwot → uwotmit DESCRIPTION Default RNG R/umap.R: ensures fully MIT-compatible dependency chain AGPL-licensed packages.","code":"# embed (original) options = list(verbose = FALSE, n_threads = 1)  # embedmit options = list(verbose = FALSE, n_threads = 1, rng_type = \"tausworthe\")"},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/index.html","id":"from-github","dir":"","previous_headings":"Installation","what":"From GitHub","title":"Extra Recipes for Encoding Predictors (MIT-Compatible Fork)","text":"Note use steps, also install packages rstanarm lme4. steps work, may want use:","code":"# install.packages(\"devtools\") devtools::install_github(\"rmsharp/embedmit\") install.packages(c(\"rpart\", \"xgboost\", \"rstanarm\", \"lme4\"))"},{"path":"https://rmsharp.github.io/embedmit/dev/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Extra Recipes for Encoding Predictors (MIT-Compatible Fork)","text":"embedmit extra steps recipes package embedding predictors one numeric columns. Almost preprocessing methods supervised. steps available separate package step dependencies, rstanarm, lme4, keras3, fairly heavy.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/index.html","id":"available-steps","dir":"","previous_headings":"Introduction","what":"Available Steps","title":"Extra Recipes for Encoding Predictors (MIT-Compatible Fork)","text":"categorical predictors: step_lencode_glm(), step_lencode_bayes(), step_lencode_mixed() estimate effect factor levels outcome estimates used new encoding. estimates estimated generalized linear model. step can executed without pooling (via glm) partial pooling (stan_glm lmer). Currently implemented numeric two-class outcomes. step_embed() uses keras3::layer_embedding translate original C factor levels set D new variables (< C). model fitting routine optimizes factor levels mapped new variables well corresponding regression coefficients (.e., neural network weights) used new encodings. step_woe() creates new variables based weight evidence encodings. numeric predictors: step_umap() uses nonlinear transformation similar t-SNE can used project transformation new data. supervised unsupervised methods can used. Note: embedmit, uses uwotmit defaults tausworthe RNG fully MIT-compatible dependency chain. step_discretize_xgb() step_discretize_cart() can make binned versions numeric predictors using supervised tree-based models. step_pca_sparse() step_pca_sparse_bayes() conduct feature extraction sparsity component loadings.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Extra Recipes for Encoding Predictors (MIT-Compatible Fork)","text":"","code":"library(embedmit) library(recipes)  # Create a recipe with UMAP embedding rec <- recipe(Species ~ ., data = iris) |>  step_normalize(all_numeric_predictors()) |>  step_umap(all_numeric_predictors(), num_comp = 2)  # Prepare and bake prepped <- prep(rec) baked <- bake(prepped, new_data = NULL)"},{"path":"https://rmsharp.github.io/embedmit/dev/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Extra Recipes for Encoding Predictors (MIT-Compatible Fork)","text":"detailed documentation embedding steps, please refer original embed documentation https://embed.tidymodels.org/. references methods : Francois C Allaire JJ (2018) Deep Learning R, Manning Guo, C Berkhahn F (2016) “Entity Embeddings Categorical Variables” Micci-Barreca D (2001) “preprocessing scheme high-cardinality categorical attributes classification prediction problems,” ACM SIGKDD Explorations Newsletter, 3(1), 27-32. Zumel N Mount J (2017) “vtreat: data.frame Processor Predictive Modeling” McInnes L Healy J (2018) UMAP: Uniform Manifold Approximation Projection Dimension Reduction Good, . J. (1985), “Weight evidence: brief survey”, Bayesian Statistics, 2, pp.249-270.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Extra Recipes for Encoding Predictors (MIT-Compatible Fork)","text":"MIT","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/index.html","id":"acknowledgments","dir":"","previous_headings":"","what":"Acknowledgments","title":"Extra Recipes for Encoding Predictors (MIT-Compatible Fork)","text":"package fork embed Emil Hvitfeldt Max Kuhn (Posit). credit core functionality goes original authors contributors.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/index.html","id":"see-also","dir":"","previous_headings":"","what":"See Also","title":"Extra Recipes for Encoding Predictors (MIT-Compatible Fork)","text":"original embed package uwotmit - MIT-compatible fork uwot sitmo instead dqrng tidymodels framework","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/add_woe.html","id":null,"dir":"Reference","previous_headings":"","what":"Add WoE in a data frame — add_woe","title":"Add WoE in a data frame — add_woe","text":"tidyverse friendly way plug WoE versions set predictor variables given binary outcome.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/add_woe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add WoE in a data frame — add_woe","text":"","code":"add_woe(.data, outcome, ..., dictionary = NULL, prefix = \"woe\")"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/add_woe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add WoE in a data frame — add_woe","text":".data tbl. data.frame plug new woe version columns. outcome bare name outcome variable. ... Bare names predictor variables, passed pass variables dplyr::select(). means can use helpers like starts_with() matches(). dictionary tbl. NULL function build dictionary variables passed .... can pass custom dictionary , see dictionary() details. prefix character string prefix resulting new variables.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/add_woe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add WoE in a data frame — add_woe","text":"tibble original columns .data plus woe columns wanted.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/add_woe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add WoE in a data frame — add_woe","text":"can pass custom dictionary add_woe(). must exactly structure output dictionary(). One easy way tweak output returned .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/add_woe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add WoE in a data frame — add_woe","text":"","code":"mtcars |> add_woe(\"am\", cyl, gear:carb) #> # A tibble: 32 × 14 #>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb woe_cyl #>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> #>  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4  0.0918 #>  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4  0.0918 #>  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1  1.36   #>  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1  0.0918 #>  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2 -1.41   #>  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1  0.0918 #>  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4 -1.41   #>  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2  1.36   #>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2  1.36   #> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4  0.0918 #> # ℹ 22 more rows #> # ℹ 2 more variables: woe_gear <dbl>, woe_carb <dbl>"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/dictionary.html","id":null,"dir":"Reference","previous_headings":"","what":"Weight of evidence dictionary — dictionary","title":"Weight of evidence dictionary — dictionary","text":"Builds woe dictionary set predictor variables upon given binary outcome. Convenient make woe version given set predictor variables also allow one tweak woe values hand.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/dictionary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weight of evidence dictionary — dictionary","text":"","code":"dictionary(.data, outcome, ..., Laplace = 1e-06)"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/dictionary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weight of evidence dictionary — dictionary","text":".data tbl. data.frame variables come . outcome bare name outcome variable exactly 2 distinct values. ... bare names predictor variables selectors accepted dplyr::select(). Laplace Default 1e-6. pseudocount parameter Laplace Smoothing estimator. Value avoid -Inf/Inf predictor category one outcome class. Set 0 allow Inf/-Inf.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/dictionary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Weight of evidence dictionary — dictionary","text":"tibble summaries woe every given predictor variable stacked .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/dictionary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Weight of evidence dictionary — dictionary","text":"can pass custom dictionary step_woe(). must exactly structure output dictionary(). One easy way tweaking output returned .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/dictionary.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Weight of evidence dictionary — dictionary","text":"Kullback, S. (1959). Information Theory Statistics. Wiley, New York. Hastie, T., Tibshirani, R. Friedman, J. (1986). Elements Statistical Learning, Second Edition, Springer, 2009. Good, . J. (1985), \"Weight evidence: brief survey\", Bayesian Statistics, 2, pp.249-270.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/dictionary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weight of evidence dictionary — dictionary","text":"","code":"mtcars |> dictionary(\"am\", cyl, gear:carb) #> # A tibble: 12 × 9 #>    variable predictor n_tot   n_0   n_1   p_0    p_1      woe outcome #>    <chr>    <chr>     <int> <dbl> <dbl> <dbl>  <dbl>    <dbl> <chr>   #>  1 cyl      4            11     3     8 0.158 0.615    1.36   am      #>  2 cyl      6             7     4     3 0.211 0.231    0.0918 am      #>  3 cyl      8            14    12     2 0.632 0.154   -1.41   am      #>  4 gear     3            15    15     0 0.789 0      -16.1    am      #>  5 gear     4            12     4     8 0.211 0.615    1.07   am      #>  6 gear     5             5     0     5 0     0.385   15.8    am      #>  7 carb     1             7     3     4 0.158 0.308    0.667  am      #>  8 carb     2            10     6     4 0.316 0.308   -0.0260 am      #>  9 carb     3             3     3     0 0.158 0      -14.5    am      #> 10 carb     4            10     7     3 0.368 0.231   -0.468  am      #> 11 carb     6             1     0     1 0     0.0769  14.2    am      #> 12 carb     8             1     0     1 0     0.0769  14.2    am"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/embedmit-package.html","id":null,"dir":"Reference","previous_headings":"","what":"embedmit: Extra Recipes for Encoding Predictors (MIT-Compatible Fork) — embedmit-package","title":"embedmit: Extra Recipes for Encoding Predictors (MIT-Compatible Fork) — embedmit-package","text":"MIT-compatible fork 'embed' package. Predictors can converted one numeric representations using variety methods. Effect encodings using simple generalized linear models doi:10.48550/arXiv.1611.09477  nonlinear models doi:10.48550/arXiv.1604.06737  can used. also functions dimension reduction approaches. fork defaults 'tausworthe' RNG UMAP avoid AGPL-licensed dependencies.","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/embedmit-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"embedmit: Extra Recipes for Encoding Predictors (MIT-Compatible Fork) — embedmit-package","text":"Maintainer: Emil Hvitfeldt emil.hvitfeldt@posit.co (ORCID) Authors: Max Kuhn max@posit.co (ORCID) contributors: Posit Software, PBC (ROR) [copyright holder, funder]","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. generics required_pkgs, tidy, tunable","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/required_pkgs.embedmit.html","id":null,"dir":"Reference","previous_headings":"","what":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_collapse_cart","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_collapse_cart","text":"Recipe-adjacent packages always list required package steps can function properly within parallel processing schemes.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/required_pkgs.embedmit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_collapse_cart","text":"","code":"# S3 method for class 'step_collapse_cart' required_pkgs(x, ...)  # S3 method for class 'step_collapse_stringdist' required_pkgs(x, ...)  # S3 method for class 'step_discretize_cart' required_pkgs(x, ...)  # S3 method for class 'step_discretize_xgb' required_pkgs(x, ...)  # S3 method for class 'step_embed' required_pkgs(x, ...)  # S3 method for class 'step_lencode' required_pkgs(x, ...)  # S3 method for class 'step_lencode_bayes' required_pkgs(x, ...)  # S3 method for class 'step_lencode_glm' required_pkgs(x, ...)  # S3 method for class 'step_lencode_mixed' required_pkgs(x, ...)  # S3 method for class 'step_pca_sparse' required_pkgs(x, ...)  # S3 method for class 'step_pca_sparse_bayes' required_pkgs(x, ...)  # S3 method for class 'step_pca_truncated' required_pkgs(x, ...)  # S3 method for class 'step_umap' required_pkgs(x, ...)  # S3 method for class 'step_woe' required_pkgs(x, ...)"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/required_pkgs.embedmit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_collapse_cart","text":"x recipe step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/required_pkgs.embedmit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_collapse_cart","text":"character vector","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/solubility.html","id":null,"dir":"Reference","previous_headings":"","what":"Compound solubility data — solubility","title":"Compound solubility data — solubility","text":"Compound solubility data","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/solubility.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Compound solubility data — solubility","text":"Tetko, ., Tanchuk, V., Kasheva, T., Villa, . (2001). Estimation aqueous solubility chemical compounds using E-state indices. Journal Chemical Information Computer Sciences, 41(6), 1488-1493. Huuskonen, J. (2000). Estimation aqueous solubility diverse set organic compounds based molecular topology. Journal Chemical Information Computer Sciences, 40(3), 773-777.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/solubility.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compound solubility data — solubility","text":"solubility data frame","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/solubility.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compound solubility data — solubility","text":"Tetko et al. (2001) Huuskonen (2000) investigated set compounds corresponding experimental solubility values using complex sets descriptors. used linear regression neural network models estimate relationship chemical structure solubility. analyses, use 1267 compounds set understandable descriptors fall one three groups: 208 binary \"fingerprints\" indicate presence absence particular chemical sub-structure, 16 count descriptors (number bonds number Bromine atoms) 4 continuous descriptors (molecular weight surface area).","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/solubility.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compound solubility data — solubility","text":"","code":"data(solubility) str(solubility) #> tibble [1,267 × 229] (S3: tbl_df/tbl/data.frame) #>  $ fp_001            : int [1:1267] 0 0 1 0 0 1 0 1 1 1 ... #>  $ fp_002            : int [1:1267] 1 1 1 0 0 0 1 0 0 1 ... #>  $ fp_003            : int [1:1267] 0 0 1 1 1 1 0 1 1 1 ... #>  $ fp_004            : int [1:1267] 0 1 1 0 1 1 1 1 1 1 ... #>  $ fp_005            : int [1:1267] 1 1 1 0 1 0 1 0 0 1 ... #>  $ fp_006            : int [1:1267] 0 1 0 0 1 0 0 0 1 1 ... #>  $ fp_007            : int [1:1267] 0 1 0 1 0 0 0 1 1 1 ... #>  $ fp_008            : int [1:1267] 1 1 1 0 0 0 1 0 0 0 ... #>  $ fp_009            : int [1:1267] 0 0 0 0 1 1 1 0 1 0 ... #>  $ fp_010            : int [1:1267] 0 0 1 0 0 0 0 0 0 0 ... #>  $ fp_011            : int [1:1267] 0 1 0 0 0 0 0 0 1 0 ... #>  $ fp_012            : int [1:1267] 0 0 0 0 0 1 0 1 0 0 ... #>  $ fp_013            : int [1:1267] 0 0 0 0 1 0 1 0 0 0 ... #>  $ fp_014            : int [1:1267] 0 0 0 0 0 0 1 0 0 0 ... #>  $ fp_015            : int [1:1267] 1 1 1 1 1 1 1 1 1 1 ... #>  $ fp_016            : int [1:1267] 0 1 0 0 1 1 0 1 0 0 ... #>  $ fp_017            : int [1:1267] 0 0 1 1 0 0 0 0 1 1 ... #>  $ fp_018            : int [1:1267] 0 1 0 0 0 0 0 0 0 0 ... #>  $ fp_019            : int [1:1267] 1 0 0 0 1 0 1 0 0 0 ... #>  $ fp_020            : int [1:1267] 0 0 0 0 0 0 0 0 0 0 ... #>  $ fp_021            : int [1:1267] 0 0 0 0 0 1 0 0 1 0 ... #>  $ fp_022            : int [1:1267] 0 0 0 0 0 0 0 0 0 1 ... #>  $ fp_023            : int [1:1267] 0 0 0 1 0 0 0 0 1 0 ... #>  $ fp_024            : int [1:1267] 1 0 0 0 1 0 0 0 0 0 ... #>  $ fp_025            : int [1:1267] 0 0 1 0 0 0 0 0 0 0 ... #>  $ fp_026            : int [1:1267] 1 0 0 0 0 0 1 0 0 0 ... #>  $ fp_027            : int [1:1267] 0 0 0 0 0 0 0 0 0 1 ... #>  $ fp_028            : int [1:1267] 0 1 0 0 0 0 0 0 1 1 ... #>  $ fp_029            : int [1:1267] 0 0 0 0 0 0 0 0 0 0 ... #>  $ fp_030            : int [1:1267] 0 0 0 0 1 0 0 0 0 0 ... #>  $ fp_031            : int [1:1267] 0 0 0 0 0 0 0 1 0 0 ... #>  $ fp_032            : int [1:1267] 0 0 0 0 0 0 0 0 0 0 ... #>  $ fp_033            : int [1:1267] 0 0 0 0 0 0 0 0 0 0 ... #>  $ fp_034            : int [1:1267] 0 0 0 0 1 0 0 0 0 1 ... #>  $ fp_035            : int [1:1267] 0 0 0 0 0 0 0 0 1 0 ... #>  $ fp_036            : int [1:1267] 0 0 0 0 0 0 0 0 0 0 ... #>  $ fp_037            : int [1:1267] 0 0 0 0 0 0 0 0 1 0 ... #>  $ fp_038            : int [1:1267] 0 0 1 0 0 0 0 0 0 0 ... #>  $ fp_039            : int [1:1267] 1 0 0 0 0 0 0 0 0 0 ... #>  $ fp_040            : int [1:1267] 1 0 0 0 0 0 0 0 0 0 ... #>  $ fp_041            : int [1:1267] 0 0 0 1 0 0 0 0 1 0 ... #>  $ fp_042            : int [1:1267] 0 0 0 0 0 0 0 0 0 0 ... #>  $ fp_043            : int [1:1267] 0 1 0 0 0 0 0 0 0 0 ... #>  $ fp_044            : int [1:1267] 0 0 0 0 0 0 0 0 0 0 ... #>  $ fp_045            : int [1:1267] 0 0 1 0 0 0 0 0 0 0 ... #>  $ fp_046            : int [1:1267] 0 1 0 0 0 0 1 0 0 1 ... #>  $ fp_047            : int [1:1267] 0 1 1 0 0 0 1 0 0 0 ... #>  $ fp_048            : int [1:1267] 0 0 0 0 0 0 0 1 0 0 ... #>  $ fp_049            : int [1:1267] 0 0 0 0 0 0 1 0 0 0 ... #>  $ fp_050            : int [1:1267] 0 0 0 0 0 0 0 1 0 1 ... #>  $ fp_051            : int [1:1267] 0 1 0 0 0 0 0 0 0 0 ... #>  $ fp_052            : int [1:1267] 0 0 0 0 0 0 0 0 0 1 ... #>  $ fp_053            : int [1:1267] 0 0 0 0 0 0 1 0 0 0 ... #>  $ fp_054            : int [1:1267] 0 0 0 1 0 0 0 0 1 1 ... #>  $ fp_055            : int [1:1267] 0 0 0 0 0 0 0 0 0 0 ... #>  $ fp_056            : int [1:1267] 1 0 0 0 0 0 0 0 0 0 ... #>  $ fp_057            : int [1:1267] 0 0 0 0 0 0 1 0 0 0 ... #>  $ fp_058            : int [1:1267] 0 0 0 0 0 0 0 0 0 1 ... #>  $ fp_059            : int [1:1267] 0 0 0 0 0 0 0 1 0 0 ... #>  $ fp_060            : int [1:1267] 0 1 1 0 0 0 0 1 1 0 ... #>  $ fp_061            : int [1:1267] 0 0 1 0 0 0 0 1 1 0 ... #>  $ fp_062            : int [1:1267] 0 0 1 0 0 1 0 1 1 1 ... #>  $ fp_063            : int [1:1267] 1 1 0 0 1 1 1 0 0 1 ... #>  $ fp_064            : int [1:1267] 0 1 1 0 1 1 0 1 0 0 ... #>  $ fp_065            : int [1:1267] 1 1 0 0 1 0 1 0 1 1 ... #>  $ fp_066            : int [1:1267] 1 0 1 1 1 1 1 1 1 1 ... #>  $ fp_067            : int [1:1267] 1 1 0 0 1 1 1 0 0 1 ... #>  $ fp_068            : int [1:1267] 0 1 0 0 1 1 1 0 0 1 ... #>  $ fp_069            : int [1:1267] 1 0 1 1 1 1 0 1 1 0 ... #>  $ fp_070            : int [1:1267] 1 1 0 1 0 0 1 0 1 0 ... #>  $ fp_071            : int [1:1267] 0 0 0 0 0 0 1 0 1 1 ... #>  $ fp_072            : int [1:1267] 0 1 1 0 0 1 0 1 1 1 ... #>  $ fp_073            : int [1:1267] 0 1 1 0 0 0 0 0 1 0 ... #>  $ fp_074            : int [1:1267] 0 1 0 0 0 0 0 0 1 0 ... #>  $ fp_075            : int [1:1267] 0 1 0 0 1 1 1 0 0 1 ... #>  $ fp_076            : int [1:1267] 1 1 0 0 0 0 1 0 1 1 ... #>  $ fp_077            : int [1:1267] 0 1 0 1 0 0 0 1 1 1 ... #>  $ fp_078            : int [1:1267] 0 1 0 0 0 0 0 0 1 0 ... #>  $ fp_079            : int [1:1267] 1 1 1 1 1 0 1 0 1 1 ... #>  $ fp_080            : int [1:1267] 0 1 0 0 1 1 1 1 0 0 ... #>  $ fp_081            : int [1:1267] 0 0 1 1 0 0 0 1 1 1 ... #>  $ fp_082            : int [1:1267] 1 1 1 0 1 1 1 0 1 1 ... #>  $ fp_083            : int [1:1267] 0 0 0 0 1 0 0 0 0 1 ... #>  $ fp_084            : int [1:1267] 1 1 0 0 1 0 1 0 0 0 ... #>  $ fp_085            : int [1:1267] 0 1 0 0 0 0 1 0 0 0 ... #>  $ fp_086            : int [1:1267] 0 0 0 1 1 0 0 1 1 1 ... #>  $ fp_087            : int [1:1267] 1 1 1 1 1 0 1 0 1 1 ... #>  $ fp_088            : int [1:1267] 0 1 0 0 0 0 0 1 1 0 ... #>  $ fp_089            : int [1:1267] 1 1 0 0 0 0 1 0 0 0 ... #>  $ fp_090            : int [1:1267] 0 1 0 1 0 0 0 1 1 1 ... #>  $ fp_091            : int [1:1267] 1 1 0 0 1 0 1 0 0 1 ... #>  $ fp_092            : int [1:1267] 0 0 0 0 1 1 1 0 1 0 ... #>  $ fp_093            : int [1:1267] 0 1 0 1 0 0 0 1 1 1 ... #>  $ fp_094            : int [1:1267] 0 0 0 0 1 0 0 1 0 0 ... #>  $ fp_095            : int [1:1267] 0 0 0 0 0 0 0 0 1 1 ... #>  $ fp_096            : int [1:1267] 0 0 0 0 0 0 0 0 1 0 ... #>  $ fp_097            : int [1:1267] 1 1 0 0 0 0 1 0 1 0 ... #>  $ fp_098            : int [1:1267] 0 0 1 0 0 0 0 1 0 0 ... #>  $ fp_099            : int [1:1267] 0 0 0 0 0 0 0 0 1 0 ... #>   [list output truncated]"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_cart.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised Collapsing of Factor Levels — step_collapse_cart","title":"Supervised Collapsing of Factor Levels — step_collapse_cart","text":"step_collapse_cart() creates specification recipe step can collapse factor levels smaller set using supervised tree.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_cart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised Collapsing of Factor Levels — step_collapse_cart","text":"","code":"step_collapse_cart(   recipe,   ...,   role = NA,   trained = FALSE,   outcome = NULL,   cost_complexity = 1e-04,   min_n = 5,   results = NULL,   skip = FALSE,   id = rand_id(\"step_collapse_cart\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_cart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised Collapsing of Factor Levels — step_collapse_cart","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections details. tidy method, currently used. role used step since new variables created. trained logical indicate quantities preprocessing estimated. outcome call vars specify variable used outcome train CART models order pool factor levels. cost_complexity non-negative value regulates complexity tree pruning occurs. Values near 0.1 usually correspond tree single splits. Values zero correspond unpruned tree. min_n integer many data points required make splits tree growing process. Larger values correspond less complex trees. results list results convert new factor levels. skip logical. step skipped recipe baked recipes::bake? operations baked recipes::prep run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_cart.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised Collapsing of Factor Levels — step_collapse_cart","text":"updated recipe step.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_cart.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Supervised Collapsing of Factor Levels — step_collapse_cart","text":"step uses CART tree (classification regression) group existing factor levels potentially smaller set. changes levels factor predictor (tidy() method can used understand translation). different ways step able collapse levels. model fails , results level split, original factor levels retained. also cases \"admissible split\" means model find signal data.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_cart.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Supervised Collapsing of Factor Levels — step_collapse_cart","text":"tidy() step, tibble returned columns terms, old, new, id: terms character, selectors variables selected old character, old levels new character, new levels id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_cart.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Supervised Collapsing of Factor Levels — step_collapse_cart","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_cart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised Collapsing of Factor Levels — step_collapse_cart","text":"","code":"data(ames, package = \"modeldata\") ames$Sale_Price <- log10(ames$Sale_Price)  rec <-   recipe(Sale_Price ~ ., data = ames) |>   step_collapse_cart(     Sale_Type, Garage_Type, Neighborhood,     outcome = vars(Sale_Price)   ) |>   prep() tidy(rec, number = 1) #> # A tibble: 45 × 4 #>    terms     old     new         id                       #>    <chr>     <chr>   <chr>       <chr>                    #>  1 Sale_Type \"ConLD\" Sale_Type_1 step_collapse_cart_EdLie #>  2 Sale_Type \"ConLw\" Sale_Type_1 step_collapse_cart_EdLie #>  3 Sale_Type \"Oth\"   Sale_Type_1 step_collapse_cart_EdLie #>  4 Sale_Type \"COD\"   Sale_Type_2 step_collapse_cart_EdLie #>  5 Sale_Type \"VWD\"   Sale_Type_2 step_collapse_cart_EdLie #>  6 Sale_Type \"ConLI\" Sale_Type_3 step_collapse_cart_EdLie #>  7 Sale_Type \"WD \"   Sale_Type_4 step_collapse_cart_EdLie #>  8 Sale_Type \"CWD\"   Sale_Type_5 step_collapse_cart_EdLie #>  9 Sale_Type \"Con\"   Sale_Type_6 step_collapse_cart_EdLie #> 10 Sale_Type \"New\"   Sale_Type_7 step_collapse_cart_EdLie #> # ℹ 35 more rows"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_stringdist.html","id":null,"dir":"Reference","previous_headings":"","what":"collapse factor levels using stringdist — step_collapse_stringdist","title":"collapse factor levels using stringdist — step_collapse_stringdist","text":"step_collapse_stringdist() creates specification recipe step collapse factor levels low stringdist .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_stringdist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"collapse factor levels using stringdist — step_collapse_stringdist","text":"","code":"step_collapse_stringdist(   recipe,   ...,   role = NA,   trained = FALSE,   distance = NULL,   method = \"osa\",   options = list(),   results = NULL,   columns = NULL,   skip = FALSE,   id = rand_id(\"collapse_stringdist\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_stringdist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"collapse factor levels using stringdist — step_collapse_stringdist","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections details. tidy method, currently used. role used step since new variables created. trained logical indicate quantities preprocessing estimated. distance Integer, value determine strings collapsed . value used inclusive, 2 collapse levels string distance 2 lower. method Character, method distance calculation. default \"osa\", see stringdist::stringdist-metrics. options List, arguments passed stringdist::stringdistmatrix() weight, q, p, bt, used different values method. results list denoting way labels collapses stored preprocessing step trained recipes::prep. columns character string variable names populated (eventually) terms argument. skip logical. step skipped recipe baked bake()? operations baked prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_stringdist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"collapse factor levels using stringdist — step_collapse_stringdist","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (columns affected) base.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_stringdist.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"collapse factor levels using stringdist — step_collapse_stringdist","text":"tidy() step, tibble returned columns terms, , , id: terms character, selectors variables selected character, old levels character, new levels id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_stringdist.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"collapse factor levels using stringdist — step_collapse_stringdist","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_collapse_stringdist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"collapse factor levels using stringdist — step_collapse_stringdist","text":"","code":"library(recipes) library(tibble) data0 <- tibble(   x1 = c(\"a\", \"b\", \"d\", \"e\", \"sfgsfgsd\", \"hjhgfgjgr\"),   x2 = c(\"ak\", \"b\", \"djj\", \"e\", \"hjhgfgjgr\", \"hjhgfgjgr\") )  rec <- recipe(~., data = data0) |>   step_collapse_stringdist(all_predictors(), distance = 1) |>   prep()  rec |>   bake(new_data = NULL) #> # A tibble: 6 × 2 #>   x1        x2        #>   <fct>     <fct>     #> 1 a         ak        #> 2 a         b         #> 3 a         djj       #> 4 a         b         #> 5 sfgsfgsd  hjhgfgjgr #> 6 hjhgfgjgr hjhgfgjgr  tidy(rec, 1) #> # A tibble: 11 × 4 #>    terms from      to        id                        #>    <chr> <chr>     <chr>     <chr>                     #>  1 x1    a         a         collapse_stringdist_PVfLS #>  2 x1    b         a         collapse_stringdist_PVfLS #>  3 x1    d         a         collapse_stringdist_PVfLS #>  4 x1    e         a         collapse_stringdist_PVfLS #>  5 x1    hjhgfgjgr hjhgfgjgr collapse_stringdist_PVfLS #>  6 x1    sfgsfgsd  sfgsfgsd  collapse_stringdist_PVfLS #>  7 x2    ak        ak        collapse_stringdist_PVfLS #>  8 x2    b         b         collapse_stringdist_PVfLS #>  9 x2    e         b         collapse_stringdist_PVfLS #> 10 x2    djj       djj       collapse_stringdist_PVfLS #> 11 x2    hjhgfgjgr hjhgfgjgr collapse_stringdist_PVfLS  rec <- recipe(~., data = data0) |>   step_collapse_stringdist(all_predictors(), distance = 2) |>   prep()  rec |>   bake(new_data = NULL) #> # A tibble: 6 × 2 #>   x1        x2        #>   <fct>     <fct>     #> 1 a         ak        #> 2 a         ak        #> 3 a         djj       #> 4 a         ak        #> 5 sfgsfgsd  hjhgfgjgr #> 6 hjhgfgjgr hjhgfgjgr  tidy(rec, 1) #> # A tibble: 11 × 4 #>    terms from      to        id                        #>    <chr> <chr>     <chr>     <chr>                     #>  1 x1    a         a         collapse_stringdist_SY1gQ #>  2 x1    b         a         collapse_stringdist_SY1gQ #>  3 x1    d         a         collapse_stringdist_SY1gQ #>  4 x1    e         a         collapse_stringdist_SY1gQ #>  5 x1    hjhgfgjgr hjhgfgjgr collapse_stringdist_SY1gQ #>  6 x1    sfgsfgsd  sfgsfgsd  collapse_stringdist_SY1gQ #>  7 x2    ak        ak        collapse_stringdist_SY1gQ #>  8 x2    b         ak        collapse_stringdist_SY1gQ #>  9 x2    e         ak        collapse_stringdist_SY1gQ #> 10 x2    djj       djj       collapse_stringdist_SY1gQ #> 11 x2    hjhgfgjgr hjhgfgjgr collapse_stringdist_SY1gQ"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_cart.html","id":null,"dir":"Reference","previous_headings":"","what":"Discretize numeric variables with CART — step_discretize_cart","title":"Discretize numeric variables with CART — step_discretize_cart","text":"step_discretize_cart() creates specification recipe step discretize numeric data (e.g. integers doubles) bins supervised way using CART model.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_cart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discretize numeric variables with CART — step_discretize_cart","text":"","code":"step_discretize_cart(   recipe,   ...,   role = NA,   trained = FALSE,   outcome = NULL,   cost_complexity = 0.01,   tree_depth = 10,   min_n = 20,   rules = NULL,   skip = FALSE,   id = rand_id(\"discretize_cart\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_cart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Discretize numeric variables with CART — step_discretize_cart","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections details. role Defaults \"predictor\". trained logical indicate quantities preprocessing estimated. outcome call vars specify variable used outcome train CART models order discretize explanatory variables. cost_complexity regularization parameter. split decrease overall lack fit factor cost_complexity attempted. Corresponds cp rpart::rpart(). Defaults 0.01. tree_depth maximum depth final tree. Corresponds maxdepth  rpart::rpart(). Defaults 10. min_n number data points node required continue splitting. Corresponds minsplit  rpart::rpart(). Defaults 20. rules splitting rules best CART tree retain variable. length zero, splitting used column. skip logical. step skipped recipe baked recipes::bake()? operations baked recipes::prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_cart.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Discretize numeric variables with CART — step_discretize_cart","text":"updated version recipe new step added sequence existing operations.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_cart.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Discretize numeric variables with CART — step_discretize_cart","text":"step_discretize_cart() creates non-uniform bins numerical variables utilizing information outcome variable applying CART model. best selection buckets variable selected using standard cost-complexity pruning CART, makes discretization method resistant overfitting. step requires rpart package. installed, step stop note installing package. Note original data replaced new bins.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_cart.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Discretize numeric variables with CART — step_discretize_cart","text":"tidy() step, tibble returned columns terms, value, id: terms character, selectors variables selected value numeric, location splits id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_cart.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Discretize numeric variables with CART — step_discretize_cart","text":"step 3 tuning parameters: cost_complexity: Cost-Complexity Parameter (type: double, default: 0.01) tree_depth: Tree Depth (type: integer, default: 10) min_n: Minimal Node Size (type: integer, default: 20)","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_cart.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Discretize numeric variables with CART — step_discretize_cart","text":"step performs supervised operation can utilize case weights. use , see documentation recipes::case_weights examples tidymodels.org.","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_cart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Discretize numeric variables with CART — step_discretize_cart","text":"","code":"library(modeldata) #>  #> Attaching package: ‘modeldata’ #> The following object is masked from ‘package:datasets’: #>  #>     penguins data(ad_data) library(rsample)  split <- initial_split(ad_data, strata = \"Class\")  ad_data_tr <- training(split) ad_data_te <- testing(split)  cart_rec <-   recipe(Class ~ ., data = ad_data_tr) |>   step_discretize_cart(     tau, age, p_tau, Ab_42,     outcome = \"Class\", id = \"cart splits\"   )  cart_rec <- prep(cart_rec, training = ad_data_tr)  # The splits: tidy(cart_rec, id = \"cart splits\") #> # A tibble: 24 × 3 #>    terms value id          #>    <chr> <dbl> <chr>       #>  1 tau   5.89  cart splits #>  2 tau   6.00  cart splits #>  3 tau   6.17  cart splits #>  4 tau   6.25  cart splits #>  5 tau   6.31  cart splits #>  6 tau   6.36  cart splits #>  7 tau   6.66  cart splits #>  8 age   0.986 cart splits #>  9 age   0.987 cart splits #> 10 age   0.988 cart splits #> # ℹ 14 more rows  bake(cart_rec, ad_data_te, tau) #> # A tibble: 84 × 1 #>    tau           #>    <fct>         #>  1 [-Inf,5.886)  #>  2 [5.995,6.175) #>  3 [5.995,6.175) #>  4 [5.995,6.175) #>  5 [-Inf,5.886)  #>  6 [-Inf,5.886)  #>  7 [6.363,6.664) #>  8 [6.175,6.249) #>  9 [-Inf,5.886)  #> 10 [6.308,6.363) #> # ℹ 74 more rows"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_xgb.html","id":null,"dir":"Reference","previous_headings":"","what":"Discretize numeric variables with XgBoost — step_discretize_xgb","title":"Discretize numeric variables with XgBoost — step_discretize_xgb","text":"step_discretize_xgb() creates specification recipe step discretize numeric data (e.g. integers doubles) bins supervised way using XgBoost model.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_xgb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discretize numeric variables with XgBoost — step_discretize_xgb","text":"","code":"step_discretize_xgb(   recipe,   ...,   role = NA,   trained = FALSE,   outcome = NULL,   sample_val = 0.2,   learn_rate = 0.3,   num_breaks = 10,   tree_depth = 1,   min_n = 5,   rules = NULL,   skip = FALSE,   id = rand_id(\"discretize_xgb\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_xgb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Discretize numeric variables with XgBoost — step_discretize_xgb","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections details. role Defaults \"predictor\". trained logical indicate quantities preprocessing estimated. outcome call vars specify variable used outcome train XgBoost models order discretize explanatory variables. sample_val Share data used validation (early stopping) learned splits (rest used training). Defaults 0.20. learn_rate rate boosting algorithm adapts iteration--iteration. Corresponds eta xgboost package. Defaults 0.3. num_breaks maximum number discrete bins bucket continuous features. Corresponds max_bin xgboost package. Defaults 10. tree_depth maximum depth tree (.e. number splits). Corresponds max_depth xgboost package. Defaults 1. min_n minimum number instances needed node. Corresponds min_child_weight xgboost package. Defaults 5. rules splitting rules best XgBoost tree retain variable. skip logical. step skipped recipe baked recipes::bake()? operations baked recipes::prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_xgb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Discretize numeric variables with XgBoost — step_discretize_xgb","text":"updated version recipe new step added sequence existing operations.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_xgb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Discretize numeric variables with XgBoost — step_discretize_xgb","text":"step_discretize_xgb() creates non-uniform bins numerical variables utilizing information outcome variable applying xgboost model. advised impute missing values step. step intended used particularly linear models thanks creating non-uniform bins becomes easier learn non-linear patterns data. best selection buckets variable selected using internal early stopping scheme implemented xgboost package, makes discretization method prone overfitting. pre-defined values underlying xgboost learns good reasonably complex results. However, one wishes tune recommended path first start changing value num_breaks e.g.: 20 30. give satisfactory results one experiment modifying tree_depth min_n parameters. Note recommended tune learn_rate simultaneously parameters. step requires xgboost package. installed, step stop note installing package. Note original data replaced new bins.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_xgb.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Discretize numeric variables with XgBoost — step_discretize_xgb","text":"tidy() step, tibble returned columns terms, value, id: terms character, selectors variables selected value numeric, location splits id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_xgb.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Discretize numeric variables with XgBoost — step_discretize_xgb","text":"step 5 tuning parameters: sample_val: Proportion data validation (type: double, default: 0.2) learn_rate: Learning Rate (type: double, default: 0.3) num_breaks: Number Cut Points (type: integer, default: 10) tree_depth: Tree Depth (type: integer, default: 1) min_n: Minimal Node Size (type: integer, default: 5)","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_xgb.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Discretize numeric variables with XgBoost — step_discretize_xgb","text":"step performs supervised operation can utilize case weights. use , see documentation recipes::case_weights examples tidymodels.org.","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_discretize_xgb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Discretize numeric variables with XgBoost — step_discretize_xgb","text":"","code":"library(rsample) library(recipes) data(credit_data, package = \"modeldata\")  set.seed(1234) split <- initial_split(credit_data[1:1000, ], strata = \"Status\")  credit_data_tr <- training(split) credit_data_te <- testing(split)  xgb_rec <-   recipe(Status ~ Income + Assets, data = credit_data_tr) |>   step_impute_median(Income, Assets) |>   step_discretize_xgb(Income, Assets, outcome = \"Status\")  xgb_rec <- prep(xgb_rec, training = credit_data_tr)  bake(xgb_rec, credit_data_te, Assets) #> # A tibble: 251 × 1 #>    Assets      #>    <fct>       #>  1 [3000,4000) #>  2 [3000,4000) #>  3 [9500, Inf] #>  4 [3000,4000) #>  5 [-Inf,2500) #>  6 [-Inf,2500) #>  7 [-Inf,2500) #>  8 [4000,4500) #>  9 [-Inf,2500) #> 10 [3000,4000) #> # ℹ 241 more rows"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_embed.html","id":null,"dir":"Reference","previous_headings":"","what":"Encoding Factors into Multiple Columns — step_embed","title":"Encoding Factors into Multiple Columns — step_embed","text":"step_embed() creates specification recipe step convert nominal (.e. factor) predictor set scores derived tensorflow model via word-embedding model. embed_control simple wrapper setting default options.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_embed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encoding Factors into Multiple Columns — step_embed","text":"","code":"step_embed(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   outcome = NULL,   predictors = NULL,   num_terms = 2,   hidden_units = 0,   options = embed_control(),   mapping = NULL,   history = NULL,   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"embed\") )  embed_control(   loss = \"mse\",   metrics = NULL,   optimizer = \"sgd\",   epochs = 20,   validation_split = 0,   batch_size = 32,   verbose = 0,   callbacks = NULL )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_embed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encoding Factors into Multiple Columns — step_embed","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables. step_embed, indicates variables encoded numeric format. See recipes::selections() details. tidy method, currently used. role model terms created step, analysis role assigned?. default, function assumes embedding variables created used predictors model. trained logical indicate quantities preprocessing estimated. outcome call vars specify variable used outcome neural network. predictors optional call vars specify variables added additional predictors neural network. variables numeric perhaps centered scaled. num_terms integer number resulting variables. hidden_units integer number hidden units dense ReLu layer embedding output later. Use value zero intermediate layer (see Details ). options list options model fitting process. mapping list tibble results define encoding. NULL step trained recipes::prep(). history tibble convergence statistics term. NULL step trained recipes::prep(). keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake()? operations baked recipes::prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify . optimizer, loss, metrics Arguments pass keras3::compile() epochs, validation_split, batch_size, verbose, callbacks Arguments pass keras3::fit()","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_embed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encoding Factors into Multiple Columns — step_embed","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables encoding), level (factor levels), several columns containing embed name.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_embed.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Encoding Factors into Multiple Columns — step_embed","text":"Factor levels initially assigned random new variables variables used neural network optimize allocation levels new columns well estimating model predict outcome. See Section 6.1.2 Francois Allaire (2018) details. new variables mapped specific levels seen time model training extra instance variables used new levels factor. One model created call step_embed. terms given step estimated encoded model also contain predictors give predictors (). outcome numeric, linear activation function used last layer softmax used factor outcomes (number levels). example, keras3 code numeric outcome, one categorical predictor, hidden units used   factor outcome used hidden units requested, code   variables specified predictors added additional dense layer layer_flatten hidden layer. Also note may difficult obtain reproducible results using step due nature Tensorflow (see link References). tensorflow models run parallel within session (via foreach futures) parallel package. using recipes step caret, avoid parallel processing.","code":"keras_model_sequential() |>   layer_embedding(     input_dim = num_factor_levels_x + 1,     output_dim = num_terms   ) |>   layer_flatten() |>   layer_dense(units = 1, activation = 'linear') keras_model_sequential() |>   layer_embedding(     input_dim = num_factor_levels_x + 1,     output_dim = num_terms   ) |>   layer_flatten() |>   layer_dense(units = hidden_units, activation = \"relu\") |>   layer_dense(units = num_factor_levels_y, activation = 'softmax')"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_embed.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Encoding Factors into Multiple Columns — step_embed","text":"tidy() step, tibble returned number columns embedding information, columns terms, levels, id: terms character, selectors variables selected levels character, levels variable id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_embed.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Encoding Factors into Multiple Columns — step_embed","text":"step 2 tuning parameters: num_terms: # Model Terms (type: integer, default: 2) hidden_units: # Hidden Units (type: integer, default: 0)","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_embed.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Encoding Factors into Multiple Columns — step_embed","text":"underlying operation allow case weights.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_embed.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Encoding Factors into Multiple Columns — step_embed","text":"Francois C Allaire JJ (2018) Deep Learning R, Manning \"Concatenate Embeddings Categorical Variables keras3\" https://flovv.github.io/Embeddings_with_keras_part2/","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_embed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encoding Factors into Multiple Columns — step_embed","text":"","code":"data(grants, package = \"modeldata\")  set.seed(1) grants_other <- sample_n(grants_other, 500)  rec <- recipe(class ~ num_ci + sponsor_code, data = grants_other) |>   step_embed(sponsor_code,     outcome = vars(class),     options = embed_control(epochs = 10)   ) #> Downloading uv... #> Done!"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_feature_hash.html","id":null,"dir":"Reference","previous_headings":"","what":"Dummy Variables Creation via Feature Hashing — step_feature_hash","title":"Dummy Variables Creation via Feature Hashing — step_feature_hash","text":"Please use textrecipes::step_dummy_hash() instead.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_feature_hash.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dummy Variables Creation via Feature Hashing — step_feature_hash","text":"","code":"step_feature_hash(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   num_hash = 2^6,   preserve = deprecated(),   columns = NULL,   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"feature_hash\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode.html","id":null,"dir":"Reference","previous_headings":"","what":"Likelihood encoding using analytical formula — step_lencode","title":"Likelihood encoding using analytical formula — step_lencode","text":"step_lencode() creates specification recipe step convert nominal (.e. factor) predictor single set scores derived analytically.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Likelihood encoding using analytical formula — step_lencode","text":"","code":"step_lencode(   recipe,   ...,   role = NA,   trained = FALSE,   outcome = NULL,   smooth = TRUE,   mapping = NULL,   skip = FALSE,   id = rand_id(\"lencode\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Likelihood encoding using analytical formula — step_lencode","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables. step_lencode(), indicates variables encoded numeric format. See recipes::selections() details. tidy method, currently used. role used step since new variables created. trained logical indicate quantities preprocessing estimated. outcome call vars specify variable used outcome. numeric two-level factors currently supported. smooth logical, default TRUE, estimates groups low counts pulled towards gobal estimate? Defaults TRUE. See Details done. also known partial pooling shrinkage. works numeric outcomes. mapping list tibble results define encoding. NULL step trained recipes::prep(). skip logical. step skipped recipe baked recipes::bake()? operations baked recipes::prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Likelihood encoding using analytical formula — step_lencode","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables encoding), level (factor levels), value (encodings).","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Likelihood encoding using analytical formula — step_lencode","text":"selected nominal predictor replaced numeric predictor. unique value nominal predictor replaced numeric value. Thse values calculated differently depending type outcome. numeric outcomes value average value outcome inside levels predictor. Unseen levels predictor using global mean predictor. case weights used weighted mean calculated instead. nominal outcomes value log odds first level outcome variable present, within level levels predictor. Unseen levels replaced global log odds without stratification. case weights used weighted log odds calculated. occurances happens log odds calculated using p = (2 * nrow(data) - 1) / (2 * nrow(data)) avoid infinity happen taking log 0. numeric outcomes smooth = TRUE, following adjustment done. $$ estimate = (n / global_{var}) / (n / global_{var} + 1 / outcome_{var}) *  estimate +  (1 / outcome_{var}) / (n / global_{var} + 1 / outcome_{var}) * global_{mean} $$ \\(n\\) number observations group.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Likelihood encoding using analytical formula — step_lencode","text":"tidy() step, tibble returned columns level, value, terms, id: level character, factor levels value numeric, encoding terms character, selectors variables selected id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Likelihood encoding using analytical formula — step_lencode","text":"step performs supervised operation can utilize case weights. use , see documentation recipes::case_weights examples tidymodels.org.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Likelihood encoding using analytical formula — step_lencode","text":"Micci-Barreca D (2001) \"preprocessing scheme high-cardinality categorical attributes classification prediction problems,\" ACM SIGKDD Explorations Newsletter, 3(1), 27-32. Zumel N Mount J (2017) \"vtreat: data.frame Processor Predictive Modeling,\" arXiv:1611.09477","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Likelihood encoding using analytical formula — step_lencode","text":"","code":"library(recipes) library(dplyr) library(modeldata)  data(grants)  set.seed(1) grants_other <- sample_n(grants_other, 500) reencoded <- recipe(class ~ sponsor_code, data = grants_other) |>   step_lencode(sponsor_code, outcome = vars(class), smooth = FALSE) |>   prep()  bake(reencoded, grants_other) #> # A tibble: 500 × 2 #>    sponsor_code class        #>           <dbl> <fct>        #>  1       -1.61  successful   #>  2        0     unsuccessful #>  3       -1.61  unsuccessful #>  4        6.91  unsuccessful #>  5        6.91  unsuccessful #>  6       -0.320 successful   #>  7        1.24  successful   #>  8        6.91  successful   #>  9       -0.320 successful   #> 10        1.24  successful   #> # ℹ 490 more rows  tidy(reencoded, 1) #> # A tibble: 80 × 4 #>    level  value terms        id            #>    <chr>  <dbl> <chr>        <chr>         #>  1 40D   -1.61  sponsor_code lencode_Y7Kxn #>  2 266B   0     sponsor_code lencode_Y7Kxn #>  3 205A   6.91  sponsor_code lencode_Y7Kxn #>  4 4D    -0.320 sponsor_code lencode_Y7Kxn #>  5 Unk    1.24  sponsor_code lencode_Y7Kxn #>  6 204D   6.91  sponsor_code lencode_Y7Kxn #>  7 2B    -0.492 sponsor_code lencode_Y7Kxn #>  8 75C    6.91  sponsor_code lencode_Y7Kxn #>  9 34B    1.20  sponsor_code lencode_Y7Kxn #> 10 113A   0.693 sponsor_code lencode_Y7Kxn #> # ℹ 70 more rows"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_bayes","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_bayes","text":"step_lencode_bayes() creates specification recipe step convert nominal (.e. factor) predictor single set scores derived generalized linear model estimated using Bayesian analysis.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_bayes","text":"","code":"step_lencode_bayes(   recipe,   ...,   role = NA,   trained = FALSE,   outcome = NULL,   options = list(seed = sample.int(10^5, 1)),   verbose = FALSE,   mapping = NULL,   skip = FALSE,   id = rand_id(\"lencode_bayes\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_bayes","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables. step_lencode_bayes, indicates variables encoded numeric format. See recipes::selections() details. tidy method, currently used. role used step since new variables created. trained logical indicate quantities preprocessing estimated. outcome call vars specify variable used outcome generalized linear model. numeric two-level factors currently supported. options list options pass rstanarm::stan_glmer(). verbose logical control default printing rstanarm::stan_glmer(). mapping list tibble results define encoding. NULL step trained recipes::prep(). skip logical. step skipped recipe baked recipes::bake()? operations baked recipes::prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_bayes","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables encoding), level (factor levels), value (encodings).","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_bayes","text":"factor predictor, generalized linear model fit outcome coefficients returned encoding. coefficients linear predictor scale , factor outcomes, log-odds units. coefficients created using intercept model , two factor outcomes used, log-odds reflect event interest first level factor. novel levels, slightly timmed average coefficients returned. hierarchical generalized linear model fit using rstanarm::stan_glmer() intercept via   ... include family argument (automatically set step, unless passed options) well arguments given options argument step. Relevant options include chains, iter, cores, arguments priors (see links References ). prior_intercept argument effect amount shrinkage.","code":"stan_glmer(outcome ~ (1 | predictor), data = data, ...)"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_bayes.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_bayes","text":"tidy() step, tibble returned columns level, value, terms, id: level character, factor levels value numeric, encoding terms character, selectors variables selected id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_bayes.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_bayes","text":"step performs supervised operation can utilize case weights. use , see documentation recipes::case_weights examples tidymodels.org.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_bayes.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_bayes","text":"Micci-Barreca D (2001) \"preprocessing scheme high-cardinality categorical attributes classification prediction problems,\" ACM SIGKDD Explorations Newsletter, 3(1), 27-32. Zumel N Mount J (2017) \"vtreat: data.frame Processor Predictive Modeling,\" arXiv:1611.09477 \"Hierarchical Partial Pooling Repeated Binary Trials\" https://CRAN.R-project.org/package=rstanarm/vignettes/pooling.html \"Prior Distributions rstanarm Models\" https://mc-stan.org/rstanarm/reference/priors.html \"Estimating Generalized (Non-)Linear Models Group-Specific Terms rstanarm\" https://mc-stan.org/rstanarm/articles/glmer.html","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_bayes","text":"","code":"library(recipes) library(dplyr) library(modeldata)  data(grants)  set.seed(1) grants_other <- sample_n(grants_other, 500) # \\donttest{ reencoded <- recipe(class ~ sponsor_code, data = grants_other) |>   step_lencode_bayes(sponsor_code, outcome = vars(class)) # }"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised Factor Conversions into Linear Functions using Likelihood Encodings — step_lencode_glm","title":"Supervised Factor Conversions into Linear Functions using Likelihood Encodings — step_lencode_glm","text":"step_lencode_glm() creates specification recipe step convert nominal (.e. factor) predictor single set scores derived generalized linear model.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised Factor Conversions into Linear Functions using Likelihood Encodings — step_lencode_glm","text":"","code":"step_lencode_glm(   recipe,   ...,   role = NA,   trained = FALSE,   outcome = NULL,   mapping = NULL,   skip = FALSE,   id = rand_id(\"lencode_glm\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised Factor Conversions into Linear Functions using Likelihood Encodings — step_lencode_glm","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables. step_lencode_glm, indicates variables encoded numeric format. See recipes::selections() details. tidy method, currently used. role used step since new variables created. trained logical indicate quantities preprocessing estimated. outcome call vars specify variable used outcome generalized linear model. numeric two-level factors currently supported. mapping list tibble results define encoding. NULL step trained recipes::prep(). skip logical. step skipped recipe baked recipes::bake()? operations baked recipes::prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised Factor Conversions into Linear Functions using Likelihood Encodings — step_lencode_glm","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables encoding), level (factor levels), value (encodings).","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_glm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Supervised Factor Conversions into Linear Functions using Likelihood Encodings — step_lencode_glm","text":"factor predictor, generalized linear model fit outcome coefficients returned encoding. coefficients linear predictor scale , factor outcomes, log-odds units. coefficients created using intercept model , two factor outcomes used, log-odds reflect event interest first level factor. novel levels, slightly timmed average coefficients returned.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_glm.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Supervised Factor Conversions into Linear Functions using Likelihood Encodings — step_lencode_glm","text":"tidy() step, tibble returned columns level, value, terms, id: level character, factor levels value numeric, encoding terms character, selectors variables selected id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_glm.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Supervised Factor Conversions into Linear Functions using Likelihood Encodings — step_lencode_glm","text":"step performs supervised operation can utilize case weights. use , see documentation recipes::case_weights examples tidymodels.org.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_glm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Supervised Factor Conversions into Linear Functions using Likelihood Encodings — step_lencode_glm","text":"Micci-Barreca D (2001) \"preprocessing scheme high-cardinality categorical attributes classification prediction problems,\" ACM SIGKDD Explorations Newsletter, 3(1), 27-32. Zumel N Mount J (2017) \"vtreat: data.frame Processor Predictive Modeling,\" arXiv:1611.09477","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised Factor Conversions into Linear Functions using Likelihood Encodings — step_lencode_glm","text":"","code":"library(recipes) library(dplyr) library(modeldata)  data(grants)  set.seed(1) grants_other <- sample_n(grants_other, 500) # \\donttest{ reencoded <- recipe(class ~ sponsor_code, data = grants_other) |>   step_lencode_glm(sponsor_code, outcome = vars(class)) # }"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_mixed.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_mixed","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_mixed","text":"step_lencode_mixed() creates specification recipe step convert nominal (.e. factor) predictor single set scores derived generalized linear mixed model.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_mixed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_mixed","text":"","code":"step_lencode_mixed(   recipe,   ...,   role = NA,   trained = FALSE,   outcome = NULL,   options = list(verbose = 0),   mapping = NULL,   skip = FALSE,   id = rand_id(\"lencode_mixed\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_mixed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_mixed","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables. step_lencode_mixed, indicates variables encoded numeric format. See recipes::selections() details. tidy method, currently used. role used step since new variables created. trained logical indicate quantities preprocessing estimated. outcome call vars specify variable used outcome generalized linear model. numeric two-level factors currently supported. options list options pass lme4::lmer() lme4::glmer(). mapping list tibble results define encoding. NULL step trained recipes::prep(). skip logical. step skipped recipe baked recipes::bake()? operations baked recipes::prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_mixed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_mixed","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables encoding), level (factor levels), value (encodings).","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_mixed.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_mixed","text":"factor predictor, generalized linear model fit outcome coefficients returned encoding. coefficients linear predictor scale , factor outcomes, log-odds units. coefficients created using intercept model , two factor outcomes used, log-odds reflect event interest first level factor. novel levels, slightly timmed average coefficients returned. hierarchical generalized linear model fit using lme4::lmer() lme4::glmer(), depending nature outcome, intercept via   ... include family argument (automatically set step) well arguments given options argument step. Relevant options include control others.","code":"lmer(outcome ~ 1 + (1 | predictor), data = data, ...)"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_mixed.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_mixed","text":"tidy() step, tibble returned columns level, value, terms, id: level character, factor levels value numeric, encoding terms character, selectors variables selected id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_mixed.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_mixed","text":"step performs supervised operation can utilize case weights. use , see documentation recipes::case_weights examples tidymodels.org.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_mixed.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_mixed","text":"Micci-Barreca D (2001) \"preprocessing scheme high-cardinality categorical attributes classification prediction problems,\" ACM SIGKDD Explorations Newsletter, 3(1), 27-32. Zumel N Mount J (2017) \"vtreat: data.frame Processor Predictive Modeling,\" arXiv:1611.09477","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_lencode_mixed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings — step_lencode_mixed","text":"","code":"library(recipes) library(dplyr) library(modeldata)  data(grants)  set.seed(1) grants_other <- sample_n(grants_other, 500) # \\donttest{ reencoded <- recipe(class ~ sponsor_code, data = grants_other) |>   step_lencode_mixed(sponsor_code, outcome = vars(class)) # }"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse PCA Signal Extraction — step_pca_sparse","title":"Sparse PCA Signal Extraction — step_pca_sparse","text":"step_pca_sparse() creates specification recipe step convert numeric data one principal components can zero coefficients.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse PCA Signal Extraction — step_pca_sparse","text":"","code":"step_pca_sparse(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   num_comp = 5,   predictor_prop = 1,   options = list(),   res = NULL,   prefix = \"PC\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"pca_sparse\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse PCA Signal Extraction — step_pca_sparse","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables used compute components. See recipes::selections details. tidy method, currently used. role model terms created step, analysis role assigned? default, function assumes new principal component columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. num_comp number components retain new predictors. num_comp greater number columns number possible components, smaller value used. num_comp = 0 set transformation done selected variables stay unchanged, regardless value keep_original_cols. predictor_prop maximum number original predictors can non-zero coefficients PCA component (via regularization). options list options default method irlba::ssvd(). res rotation matrix preprocessing step trained recipes::prep. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake()? operations baked recipes::prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse PCA Signal Extraction — step_pca_sparse","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected), value (loading), component.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparse PCA Signal Extraction — step_pca_sparse","text":"irlba package required step. installed, user prompted step defined. irlba::ssvd() function used encourage sparsity; documentation details method. argument num_comp controls number components retained (original variables used derive components removed data). new components names begin prefix sequence numbers. variable names padded zeros. example, num_comp < 10, names PC1 - PC9. num_comp = 101, names PC1 - PC101.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Sparse PCA Signal Extraction — step_pca_sparse","text":"tidy() step, tibble returned columns terms, value, component, id: terms character, selectors variables selected value numeric, variable loading component character, principle component id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Sparse PCA Signal Extraction — step_pca_sparse","text":"step 2 tuning parameters: num_comp: # Components (type: integer, default: 5) predictor_prop: Proportion Predictors (type: double, default: 1)","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Sparse PCA Signal Extraction — step_pca_sparse","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparse PCA Signal Extraction — step_pca_sparse","text":"","code":"library(recipes) library(ggplot2)  data(ad_data, package = \"modeldata\")  ad_rec <-   recipe(Class ~ ., data = ad_data) |>   step_zv(all_predictors()) |>   step_YeoJohnson(all_numeric_predictors()) |>   step_normalize(all_numeric_predictors()) |>   step_pca_sparse(     all_numeric_predictors(),     predictor_prop = 0.75,     num_comp = 3,     id = \"sparse pca\"   ) |>   prep()  tidy(ad_rec, id = \"sparse pca\") |>   mutate(value = ifelse(value == 0, NA, value)) |>   ggplot(aes(x = component, y = terms, fill = value)) +   geom_tile() +   scale_fill_gradient2() +   theme(axis.text.y = element_blank())"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse Bayesian PCA Signal Extraction — step_pca_sparse_bayes","title":"Sparse Bayesian PCA Signal Extraction — step_pca_sparse_bayes","text":"step_pca_sparse_bayes() creates specification recipe step convert numeric data one principal components can zero coefficients.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse Bayesian PCA Signal Extraction — step_pca_sparse_bayes","text":"","code":"step_pca_sparse_bayes(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   num_comp = 5,   prior_slab_dispersion = 1,   prior_mixture_threshold = 0.1,   options = list(),   res = NULL,   prefix = \"PC\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"pca_sparse_bayes\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse Bayesian PCA Signal Extraction — step_pca_sparse_bayes","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables used compute components. See recipes::selections details. tidy method, currently used. role model terms created step, analysis role assigned? default, function assumes new principal component columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. num_comp number components retain new predictors. num_comp greater number columns number possible components, smaller value used. num_comp = 0 set transformation done selected variables stay unchanged, regardless value keep_original_cols. prior_slab_dispersion value proportional dispersion (scale) parameter slab portion prior. Smaller values result increase zero coefficients. prior_mixture_threshold parameter defines trade-spike slab components prior. Increasing parameter increases number zero coefficients. options list options default method VBsparsePCA::VBsparsePCA(). res rotation matrix preprocessing step trained recipes::prep. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake()? operations baked recipes::prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse Bayesian PCA Signal Extraction — step_pca_sparse_bayes","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms (selectors variables selected), value (loading), component.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparse Bayesian PCA Signal Extraction — step_pca_sparse_bayes","text":"VBsparsePCA package required step. installed, user prompted step defined. spike--slab prior mixture two priors. One (\"spike\") mass zero represents variable contribution PCA coefficients. prior broader distribution reflects coefficient distribution variables affect PCA analysis. \"slab\". narrower slab, likely coefficient zero (regularized closer zero). mixture two priors governed mixing parameter, prior distribution hyper-parameter prior. PCA coefficients resulting scores unique sign. step attempt make sign components consistent run--run. However, sparsity constraint may interfere goal. argument num_comp controls number components retained (original variables used derive components removed data). new components names begin prefix sequence numbers. variable names padded zeros. example, num_comp < 10, names PC1 - PC9. num_comp = 101, names PC1 - PC101.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse_bayes.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Sparse Bayesian PCA Signal Extraction — step_pca_sparse_bayes","text":"tidy() step, tibble returned columns terms, value, component, id: terms character, selectors variables selected value numeric, variable loading component character, principle component id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse_bayes.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Sparse Bayesian PCA Signal Extraction — step_pca_sparse_bayes","text":"step 3 tuning parameters: num_comp: # Components (type: integer, default: 5) prior_slab_dispersion: Dispersion Slab Prior (type: double, default: 1) prior_mixture_threshold: Threshold Mixture Prior (type: double, default: 0.1)","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse_bayes.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Sparse Bayesian PCA Signal Extraction — step_pca_sparse_bayes","text":"underlying operation allow case weights.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse_bayes.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sparse Bayesian PCA Signal Extraction — step_pca_sparse_bayes","text":"Ning, B. (2021). Spike slab Bayesian sparse principal component analysis. arXiv:2102.00305.","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_sparse_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparse Bayesian PCA Signal Extraction — step_pca_sparse_bayes","text":"","code":"library(recipes) library(ggplot2)  data(ad_data, package = \"modeldata\")  ad_rec <-   recipe(Class ~ ., data = ad_data) |>   step_zv(all_predictors()) |>   step_YeoJohnson(all_numeric_predictors()) |>   step_normalize(all_numeric_predictors()) |>   step_pca_sparse_bayes(     all_numeric_predictors(),     prior_mixture_threshold = 0.95,     prior_slab_dispersion = 0.05,     num_comp = 3,     id = \"sparse bayesian pca\"   ) |>   prep()  tidy(ad_rec, id = \"sparse bayesian pca\") |>   mutate(value = ifelse(value == 0, NA, value)) |>   ggplot(aes(x = component, y = terms, fill = value)) +   geom_tile() +   scale_fill_gradient2() +   theme(axis.text.y = element_blank())"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_truncated.html","id":null,"dir":"Reference","previous_headings":"","what":"Truncated PCA Signal Extraction — step_pca_truncated","title":"Truncated PCA Signal Extraction — step_pca_truncated","text":"step_pca_truncated() creates specification recipe step convert numeric data one principal components. truncated calculates number components asked instead done recipes::step_pca().","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_truncated.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Truncated PCA Signal Extraction — step_pca_truncated","text":"","code":"step_pca_truncated(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   num_comp = 5,   options = list(),   res = NULL,   columns = NULL,   prefix = \"PC\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"pca_truncated\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_truncated.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Truncated PCA Signal Extraction — step_pca_truncated","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables step. See selections() details. role model terms created step, analysis role assigned? default, new columns created step original variables used predictors model. trained logical indicate quantities preprocessing estimated. num_comp number components retain new predictors. num_comp greater number columns number possible components, smaller value used. num_comp = 0 set transformation done selected variables stay unchanged, regardless value keep_original_cols. options list options default method irlba::prcomp_irlba(). Argument defaults set retx = FALSE, center = FALSE, scale. = FALSE, tol = NULL. Note argument x passed (). res irlba::prcomp_irlba() object stored preprocessing step trained recipes::prep. columns character string selected variable names. field placeholder populated prep() used. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked bake()? operations baked prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_truncated.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Truncated PCA Signal Extraction — step_pca_truncated","text":"updated version recipe new step added sequence existing operations.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_truncated.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Truncated PCA Signal Extraction — step_pca_truncated","text":"Principal component analysis (PCA) transformation group variables produces new set artificial features components. components designed capture maximum amount information (.e. variance) original variables. Also, components statistically independent one another. means can used combat large inter-variables correlations data set. advisable standardize variables prior running PCA. , variable centered scaled prior PCA calculation. can changed using options argument using recipes::step_center() recipes::step_scale(). argument num_comp controls number components retained (original variables used derive components removed data). new components names begin prefix sequence numbers. variable names padded zeros. example, num_comp < 10, names PC1 - PC9. num_comp = 101, names PC1 - PC101.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_truncated.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Truncated PCA Signal Extraction — step_pca_truncated","text":"tidy() step two things can happen depending type argument. type = \"coef\" tibble returned 4 columns terms, value, component , id: terms character, selectors variables selected value numeric, variable loading component character, principle component id character, id step type = \"variance\" tibble returned 4 columns terms, value, component , id: terms character, type variance value numeric, value variance component integer, principle component id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_truncated.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Truncated PCA Signal Extraction — step_pca_truncated","text":"step 1 tuning parameters: num_comp: # Components (type: integer, default: 5)","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_truncated.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Truncated PCA Signal Extraction — step_pca_truncated","text":"step performs unsupervised operation can utilize case weights. result, case weights used frequency weights. information, see documentation recipes::case_weights examples tidymodels.org.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_truncated.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Truncated PCA Signal Extraction — step_pca_truncated","text":"Jolliffe, . T. (2010). Principal Component Analysis. Springer.","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_pca_truncated.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Truncated PCA Signal Extraction — step_pca_truncated","text":"","code":"rec <- recipe(~., data = mtcars) pca_trans <- rec |>   step_normalize(all_numeric()) |>   step_pca_truncated(all_numeric(), num_comp = 2) pca_estimates <- prep(pca_trans, training = mtcars) pca_data <- bake(pca_estimates, mtcars)  rng <- extendrange(c(pca_data$PC1, pca_data$PC2)) plot(pca_data$PC1, pca_data$PC2,   xlim = rng, ylim = rng )   tidy(pca_trans, number = 2) #> # A tibble: 1 × 4 #>   terms         value component id                  #>   <chr>         <dbl> <chr>     <chr>               #> 1 all_numeric()    NA NA        pca_truncated_H3Jtu tidy(pca_estimates, number = 2) #> # A tibble: 22 × 4 #>    terms  value component id                  #>    <chr>  <dbl> <chr>     <chr>               #>  1 mpg    0.363 PC1       pca_truncated_H3Jtu #>  2 cyl   -0.374 PC1       pca_truncated_H3Jtu #>  3 disp  -0.368 PC1       pca_truncated_H3Jtu #>  4 hp    -0.330 PC1       pca_truncated_H3Jtu #>  5 drat   0.294 PC1       pca_truncated_H3Jtu #>  6 wt    -0.346 PC1       pca_truncated_H3Jtu #>  7 qsec   0.200 PC1       pca_truncated_H3Jtu #>  8 vs     0.307 PC1       pca_truncated_H3Jtu #>  9 am     0.235 PC1       pca_truncated_H3Jtu #> 10 gear   0.207 PC1       pca_truncated_H3Jtu #> # ℹ 12 more rows"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_umap.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","title":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","text":"step_umap() creates specification recipe step project set features smaller space.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_umap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","text":"","code":"step_umap(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   outcome = NULL,   neighbors = 15,   num_comp = 2,   min_dist = 0.01,   metric = \"euclidean\",   learn_rate = 1,   epochs = NULL,   initial = \"spectral\",   target_weight = 0.5,   options = list(verbose = FALSE, n_threads = 1, rng_type = \"tausworthe\"),   seed = sample(10^5, 2),   prefix = \"UMAP\",   keep_original_cols = FALSE,   retain = deprecated(),   object = NULL,   skip = FALSE,   id = rand_id(\"umap\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_umap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables step. See selections() details. role model terms created step, analysis role assigned? default, new columns created step original variables used predictors model. trained logical indicate quantities preprocessing estimated. outcome call vars specify variable used outcome encoding process (). neighbors integer number nearest neighbors used construct target simplicial set. neighbors greater number data points, smaller value used. num_comp integer number UMAP components. num_comp greater number selected columns minus one, smaller value used. min_dist effective minimum distance embedded points. metric Character, type distance metric use find nearest neighbors. See uwotmit::umap() details. Default \"euclidean\". learn_rate Positive number learning rate optimization process. epochs Number iterations neighbor optimization. See uwotmit::umap() details. initial Character, Type initialization coordinates. Can one \"spectral\", \"normlaplacian\", \"random\", \"lvrandom\", \"laplacian\", \"pca\", \"spca\", \"agspectral\", matrix initial coordinates. See uwotmit::umap() details. Default \"spectral\". target_weight Weighting factor data topology target topology. value 0.0 weights entirely data, value 1.0 weights entirely target. default 0.5 balances weighting equally data target. options list options pass uwotmit::umap(). arguments X, n_neighbors, n_components, min_dist, n_epochs, ret_model, learning_rate passed . default, verbose n_threads set. seed Two integers control random numbers used numerical methods. default pulls main session's stream numbers give reproducible results seed set prior calling recipes::prep recipes::bake. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. retain Use keep_original_cols instead specify whether original predictors retained along new embedding variables. object object defines encoding. NULL step trained recipes::prep(). skip logical. step skipped recipe baked bake()? operations baked prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_umap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","text":"updated version recipe new step added sequence existing operations.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_umap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","text":"UMAP, short Uniform Manifold Approximation Projection, nonlinear dimension reduction technique finds local, low-dimensional representations data. can run unsupervised supervised different types outcome data (e.g. numeric, factor, etc). argument num_comp controls number components retained (original variables used derive components removed data). new components names begin prefix sequence numbers. variable names padded zeros. example, num_comp < 10, names UMAP1 - UMAP9. num_comp = 101, names UMAP1 - UMAP101.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_umap.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","text":"tidy() step, tibble returned columns terms id: terms character, selectors variables selected id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_umap.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","text":"step 7 tuning parameters: num_comp: # Components (type: integer, default: 2) neighbors: # Nearest Neighbors (type: integer, default: 15) min_dist: Min Distance Points (type: double, default: 0.01) learn_rate: Learning Rate (type: double, default: 1) epochs: # Epochs (type: integer, default: NULL) initial: UMAP Initialization (type: character, default: spectral) target_weight: Proportion Supervised (type: double, default: 0.5)","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_umap.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","text":"underlying operation allow case weights.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_umap.html","id":"saving-prepped-recipe-object","dir":"Reference","previous_headings":"","what":"Saving prepped recipe object","title":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","text":"recipe step may require native serialization saving use another R session. learn serialization prepped recipes, see bundle package.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_umap.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","text":"McInnes, L., & Healy, J. (2018). UMAP: Uniform Manifold Approximation Projection Dimension Reduction. https://arxiv.org/abs/1802.03426. \"UMAP Works\" https://umap-learn.readthedocs.io/en/latest/how_umap_works.html","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_umap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised and unsupervised uniform manifold approximation and projection (UMAP) — step_umap","text":"","code":"library(recipes) library(ggplot2)  split <- seq.int(1, 150, by = 9) tr <- iris[-split, ] te <- iris[split, ]  set.seed(11) supervised <-   recipe(Species ~ ., data = tr) |>   step_center(all_predictors()) |>   step_scale(all_predictors()) |>   step_umap(all_predictors(), outcome = vars(Species), num_comp = 2) |>   prep(training = tr)  theme_set(theme_bw())  bake(supervised, new_data = te, Species, starts_with(\"umap\")) |>   ggplot(aes(x = UMAP1, y = UMAP2, col = Species)) +   geom_point(alpha = .5)"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_woe.html","id":null,"dir":"Reference","previous_headings":"","what":"Weight of evidence transformation — step_woe","title":"Weight of evidence transformation — step_woe","text":"step_woe() creates specification recipe step transform nominal data numerical transformation based weights evidence binary outcome.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_woe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weight of evidence transformation — step_woe","text":"","code":"step_woe(   recipe,   ...,   role = \"predictor\",   outcome,   trained = FALSE,   dictionary = NULL,   Laplace = 1e-06,   prefix = \"woe\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"woe\") )"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_woe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weight of evidence transformation — step_woe","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables used compute components. See recipes::selections details. tidy method, currently used. role model terms created step, analysis role assigned?. default, function assumes new woe components columns created original variables used predictors model. outcome bare name binary outcome encased vars(). trained logical indicate quantities preprocessing estimated. dictionary tbl. map levels woe values. must layout output returned dictionary(). NULL function build dictionary variables passed .... See dictionary() details. Laplace Laplace smoothing parameter. value usually applied avoid -Inf/Inf predictor category one outcome class. Set 0 allow Inf/-Inf. default 1e-6. Also known 'pseudocount' parameter Laplace smoothing technique. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake()? operations baked recipes::prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations id character string unique step identify .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_woe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Weight of evidence transformation — step_woe","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble woe dictionary used map categories woe values.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_woe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Weight of evidence transformation — step_woe","text":"WoE transformation group variables produces new set features. formula $$woe_c = log((P(X = c|Y = 1))/(P(X = c|Y = 0)))$$ \\(c\\) goes 1 \\(C\\) levels given nominal predictor variable \\(X\\). components designed transform nominal variables numerical ones property order magnitude reflects association binary outcome.  apply numerical predictors, advisable discretize variables prior running WoE. , variable binarized woe associated later. can achieved using recipes::step_discretize(). argument Laplace small quantity added proportions 1's 0's goal avoid log(p/0) log(0/p) results. numerical woe versions names begin woe_ followed respective original name variables. See Good (1985). One can pass custom dictionary tibble step_woe(). must structure output dictionary() (see examples). provided created automatically. role tibble store map levels nominal predictor woe values. may want tweak object goal fix orders levels one given predictor. One easy way tweaking output returned dictionary().","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_woe.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Weight of evidence transformation — step_woe","text":"tidy() step, tibble returned columns terms, value, n_tot, n_bad, n_good, p_bad, p_good, woe, outcome, id: terms character, selectors variables selected value character, level outcome n_tot integer, total number n_bad integer, number bad examples n_good integer, number good examples p_bad numeric, p bad examples p_good numeric, p good examples woe numeric, weight evidence outcome character, name outcome variable id character, id step","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_woe.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Weight of evidence transformation — step_woe","text":"step 1 tuning parameters: Laplace: Laplace Correction (type: double, default: 1e-06)","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_woe.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Weight of evidence transformation — step_woe","text":"underlying operation allow case weights.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_woe.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Weight of evidence transformation — step_woe","text":"Kullback, S. (1959). Information Theory Statistics. Wiley, New York. Hastie, T., Tibshirani, R. Friedman, J. (1986). Elements Statistical Learning, Second Edition, Springer, 2009. Good, . J. (1985), \"Weight evidence: brief survey\", Bayesian Statistics, 2, pp.249-270.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/step_woe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weight of evidence transformation — step_woe","text":"","code":"library(modeldata) data(\"credit_data\")  set.seed(111) in_training <- sample(1:nrow(credit_data), 2000)  credit_tr <- credit_data[in_training, ] credit_te <- credit_data[-in_training, ]  rec <- recipe(Status ~ ., data = credit_tr) |>   step_woe(Job, Home, outcome = vars(Status))  woe_models <- prep(rec, training = credit_tr) #> Warning: Some columns used by `step_woe()` have categories with fewer than 10 values: #> \"Home\" and \"Job\"  # the encoding: bake(woe_models, new_data = credit_te |> slice(1:5), starts_with(\"woe\")) #> # A tibble: 5 × 2 #>   woe_Job woe_Home #>     <dbl>    <dbl> #> 1  -0.451   0.519  #> 2   0.187  -0.512  #> 3  -0.451  -0.512  #> 4   0.187  -0.512  #> 5   1.51   -0.0519 # the original data credit_te |>   slice(1:5) |>   dplyr::select(Job, Home) #>         Job    Home #> 1     fixed    rent #> 2 freelance   owner #> 3     fixed   owner #> 4 freelance   owner #> 5   partime parents # the details: tidy(woe_models, number = 1) #> # A tibble: 12 × 10 #>    terms value     n_tot n_bad n_good   p_bad  p_good     woe outcome id        #>    <chr> <chr>     <int> <dbl>  <dbl>   <dbl>   <dbl>   <dbl> <chr>   <chr>     #>  1 Job   fixed      1261   273    988 0.451   0.708   -0.451  Status  woe_omZtf #>  2 Job   freelance   463   159    304 0.263   0.218    0.187  Status  woe_omZtf #>  3 Job   others       74    39     35 0.0645  0.0251   0.944  Status  woe_omZtf #>  4 Job   partime     201   133     68 0.220   0.0487   1.51   Status  woe_omZtf #>  5 Job   NA            1     1      0 0.00165 0       14.7    Status  woe_omZtf #>  6 Home  ignore        8     4      4 0.00661 0.00287  0.835  Status  woe_omZtf #>  7 Home  other       161    78     83 0.129   0.0595   0.773  Status  woe_omZtf #>  8 Home  owner       931   192    739 0.317   0.530   -0.512  Status  woe_omZtf #>  9 Home  parents     336    98    238 0.162   0.171   -0.0519 Status  woe_omZtf #> 10 Home  priv        113    42     71 0.0694  0.0509   0.310  Status  woe_omZtf #> 11 Home  rent        446   188    258 0.311   0.185    0.519  Status  woe_omZtf #> 12 Home  NA            5     3      2 0.00496 0.00143  1.24   Status  woe_omZtf  # Example of custom dictionary + tweaking # custom dictionary woe_dict_custom <- credit_tr |> dictionary(Job, Home, outcome = \"Status\") woe_dict_custom[4, \"woe\"] <- 1.23 # tweak  # passing custom dict to step_woe() rec_custom <- recipe(Status ~ ., data = credit_tr) |>   step_woe(     Job, Home,     outcome = vars(Status), dictionary = woe_dict_custom   ) |>   prep() #> Warning: Some columns used by `step_woe()` have categories with fewer than 10 values: #> \"Home\" and \"Job\"  rec_custom_baked <- bake(rec_custom, new_data = credit_te) rec_custom_baked |>   dplyr::filter(woe_Job == 1.23) |>   head() #> # A tibble: 6 × 14 #>   Seniority  Time   Age Marital Records Expenses Income Assets  Debt Amount #>       <int> <int> <int> <fct>   <fct>      <int>  <int>  <int> <int>  <int> #> 1         0    48    41 married no            90     80      0     0   1200 #> 2         0    18    21 single  yes           35     50      0     0    400 #> 3         0    36    23 single  no            45    122   2500     0    400 #> 4        14    24    51 married no            75    198   1000     0    450 #> 5         1    60    26 single  no            35    120      0     0   1150 #> 6         1    36    24 married no            76    164      0     0    900 #> # ℹ 4 more variables: Price <int>, Status <fct>, woe_Job <dbl>, woe_Home <dbl>"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/tunable_embedmit.html","id":null,"dir":"Reference","previous_headings":"","what":"tunable methods for embedmit — tunable.step_discretize_cart","title":"tunable methods for embedmit — tunable.step_discretize_cart","text":"functions define parameters can tuned specific steps. also define recommended objects dials package can used generate new parameter values characteristics.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/tunable_embedmit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"tunable methods for embedmit — tunable.step_discretize_cart","text":"","code":"# S3 method for class 'step_discretize_cart' tunable(x, ...)  # S3 method for class 'step_discretize_xgb' tunable(x, ...)  # S3 method for class 'step_embed' tunable(x, ...)  # S3 method for class 'step_pca_sparse' tunable(x, ...)  # S3 method for class 'step_pca_sparse_bayes' tunable(x, ...)  # S3 method for class 'step_umap' tunable(x, ...)  # S3 method for class 'step_woe' tunable(x, ...)"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/tunable_embedmit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"tunable methods for embedmit — tunable.step_discretize_cart","text":"x recipe step object ... used.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/tunable_embedmit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"tunable methods for embedmit — tunable.step_discretize_cart","text":"tibble object.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/woe_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Crosstable with woe between a binary outcome and a predictor variable. — woe_table","title":"Crosstable with woe between a binary outcome and a predictor variable. — woe_table","text":"Calculates summaries WoE (Weight Evidence) binary outcome given predictor variable. Used biuld dictionary.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/woe_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Crosstable with woe between a binary outcome and a predictor variable. — woe_table","text":"","code":"woe_table(predictor, outcome, Laplace = 1e-06, call = rlang::caller_env(0))"},{"path":"https://rmsharp.github.io/embedmit/dev/reference/woe_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Crosstable with woe between a binary outcome and a predictor variable. — woe_table","text":"predictor atomic vector, usualy distinct values. outcome dependent variable. atomic vector exactly 2 distinct values. Laplace pseudocount parameter Laplace Smoothing estimator. Default 1e-6. Value avoid -Inf/Inf predictor category one outcome class. Set 0 allow Inf/-Inf. call execution environment currently running function, e.g. caller_env(). function mentioned error messages source error. See call argument rlang::abort() information.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/woe_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Crosstable with woe between a binary outcome and a predictor variable. — woe_table","text":"tibble counts, proportions woe. Warning: woe can possibly -Inf. Use 'Laplace' arg avoid .","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/reference/woe_table.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Crosstable with woe between a binary outcome and a predictor variable. — woe_table","text":"Kullback, S. (1959). Information Theory Statistics. Wiley, New York. Hastie, T., Tibshirani, R. Friedman, J. (1986). Elements Statistical Learning, Second Edition, Springer, 2009. Good, . J. (1985), \"Weight evidence: brief survey\", Bayesian Statistics, 2, pp.249-270.","code":""},{"path":[]},{"path":"https://rmsharp.github.io/embedmit/dev/news/index.html","id":"initial-release-1-0-0","dir":"Changelog","previous_headings":"","what":"Initial Release","title":"embedmit 1.0.0","text":"first release embedmit, MIT-compatible fork embed package (version 1.2.1) Emil Hvitfeldt Max Kuhn (Posit Software, PBC).","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/news/index.html","id":"key-differences-from-embed-1-0-0","dir":"Changelog","previous_headings":"Initial Release","what":"Key Differences from embed","title":"embedmit 1.0.0","text":"MIT-Compatible: embedmit depends uwotmit instead uwot UMAP functionality. avoids AGPL-licensed dqrng dependency uwot uses, making embedmit suitable inclusion projects requiring permissive licensing. Default RNG: step_umap() function defaults rng_type = \"tausworthe\" instead \"pcg\" avoid AGPL-licensed PCG implementation. Users can still explicitly request rng_type = \"pcg\" desired, use MIT-licensed sitmo implementation uwotmit. Package renamed: package named embedmit distinguish original embed package.","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/news/index.html","id":"inherited-features-from-embed-1-0-0","dir":"Changelog","previous_headings":"Initial Release","what":"Inherited Features from embed 1.2.1","title":"embedmit 1.0.0","text":"embedmit inherits features embed 1.2.1, including: step_umap() UMAP dimensionality reduction step_lencode_glm(), step_lencode_bayes(), step_lencode_mixed() likelihood encodings step_lencode() analytical likelihood encoding optional smoothing step_woe() weight evidence encodings step_embed() entity embeddings using neural networks step_discretize_cart() step_discretize_xgb() supervised binning step_collapse_cart() step_collapse_stringdist() factor level pooling step_pca_sparse(), step_pca_sparse_bayes(), step_pca_truncated() sparse PCA methods Full integration tidymodels ecosystem","code":""},{"path":"https://rmsharp.github.io/embedmit/dev/news/index.html","id":"acknowledgments-1-0-0","dir":"Changelog","previous_headings":"Initial Release","what":"Acknowledgments","title":"embedmit 1.0.0","text":"package based embed Emil Hvitfeldt, Max Kuhn, Posit Software, PBC. credit recipe step implementations goes original authors. fork exists solely provide MIT-compatible alternative users require permissive licensing. original embed package, see: https://github.com/tidymodels/embed","code":""}]
